{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'politifact_clean': 'data/truth-detectiondeception-detectionlie-detection/politifact_clean.csv', 'politifact_clean_binarized': 'data/truth-detectiondeception-detectionlie-detection/politifact_clean_binarized.csv'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "from torch.nn import functional as f\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "DIRECTORY = 'data'\n",
    "paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(DIRECTORY):\n",
    "    for name in files:\n",
    "        paths.append(os.path.join(root, name))\n",
    "\n",
    "names = [i.split('/')[-1] for i in paths][1:]\n",
    "data_dict = dict(zip([i[:-4] for i in names], paths[1:]))\n",
    "print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from newspaper import Article\n",
    "import newspaper\n",
    "\n",
    "def meta_extract(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.download('punkt')\n",
    "    article.nlp()\n",
    "\n",
    "    return article.authors, article.publish_date, article.top_image, article.images, article.title, article.summary\n",
    "\n",
    "# meta_extract(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer, SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "import nltk\n",
    "\n",
    "def sentiment(inp_text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return sia.polarity_scores(inp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The various offices of state government have purchased more than $11 million in vehicles, not counting leases, for use from the Governor on down during the last two years.\"\n",
      "173\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   1   3  21  81  12  18  16 580  42   2  27   8 259  22   1 157  11\n",
      " 128 197   1  84 116  34]\n",
      "{1: 0, 0: 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-9a7e6d435e4c>:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column][idx] = torch.tensor(vectorized, dtype=float)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import nltk\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "\n",
    "\n",
    "class PreprocessingDataset(Dataset):\n",
    "    def __init__(self, file, root, x_col, y_col, meta_columns, label_idx = -1):\n",
    "        self.x_col = x_col\n",
    "        self.y_col = y_col\n",
    "        self.data = pd.read_csv(file)\n",
    "        self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "        self.data = self.data.drop(meta_columns, axis=1)\n",
    "\n",
    "        # self.data, self.base_ref = self.tokenizer(self.data, [x_col])\n",
    "        self.x_data = self.data[x_col]\n",
    "        self.max_len = max([len(i) for i in self.x_data])\n",
    "        self.max_len = 600\n",
    "\n",
    "        self.x_data, self.token = self.word_vector(self.x_data)\n",
    "\n",
    "        # list_x = self.x_data.tolist()\n",
    "        # for x, entry in enumerate(list_x):\n",
    "        #     new_entry = self.format_text(entry)\n",
    "        #     list_x[x] = new_entry\n",
    "\n",
    "        \n",
    "        # train_docs = ' '.join(map(str, list_x))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        self.data[x_col] = [torch.tensor(i) for i in self.x_data]\n",
    "        self.data = self.vectorize(self.data, [y_col])\n",
    "        self.df_data = self.data\n",
    "        self.data = self.data.to_numpy()\n",
    "\n",
    "        self.root = root\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def format_text(self, token):\n",
    "        clean_token = ''.join(chr for chr in token if chr.isalnum() and chr.isalpha())\n",
    "        return clean_token\n",
    "\n",
    "    def word_vector(self, data):\n",
    "        x_data = data\n",
    "        x_data = list(x_data)\n",
    "        maximum_length = 0\n",
    "        max_idx = 0\n",
    "        for idx, i in enumerate(x_data):\n",
    "\n",
    "            if len(i) > maximum_length:\n",
    "                maximum_length = len(i)\n",
    "                max_idx = idx\n",
    "        maximum_length = 600\n",
    "        t = Tokenizer(num_words=600, filters='\\n.,:!\"#$()&@%^()-_`~[];.,{|}')\n",
    "        t.fit_on_texts(x_data)\n",
    "        sequences = t.texts_to_sequences(x_data)\n",
    "        sequences = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=maximum_length)\n",
    "        print(x_data[0])\n",
    "        print(len(x_data[0]))\n",
    "        print(sequences[0])\n",
    "\n",
    "        return sequences, t\n",
    "\n",
    "\n",
    "    def vectorize(self, data_inp, columns):\n",
    "        data = data_inp\n",
    "        for column in columns:\n",
    "            labels = list(data[column].unique())\n",
    "            ref = dict(zip(data[column].unique(), [i for i in range(len(labels))]))\n",
    "            print(ref)\n",
    "            for idx, val in enumerate(data[column]):\n",
    "                vectorized = ref[data[column][idx]]\n",
    "                data[column][idx] = torch.tensor(vectorized, dtype=float)\n",
    "        return data\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__ (self, idx):\n",
    "        \n",
    "        self.transpose_data = self.data\n",
    "        self.transpose_data = self.transpose_data.transpose()\n",
    "        x_data = self.transpose_data[0]\n",
    "        y_data = self.transpose_data[1]\n",
    "\n",
    "        return x_data[idx], y_data[idx]\n",
    "\n",
    "clean_truth_data = PreprocessingDataset(data_dict['politifact_clean_binarized'], DIRECTORY, 'statement', 'veracity', ['source', 'link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MostuxdrecentlyuxdTrumpuxdhasuxdcalleduxdtheuxdSARSCoVuxdcoronavirusuxdtheuxdChineseuxdvirusuxdanduxdkunguxdfluuxduxdracistuxdtermsuxdthatuxdtapuxdintouxdtheuxdkinduxdofuxdxenophobiauxdthatuxdheuxdlatcheduxdontouxdduringuxdhisuxduxdpresidentialuxdcampaignuxdTrumpsuxdownuxdadviseruxdKellyanneuxdConwayuxdpreviouslyuxdcalleduxdkunguxdfluuxdauxdhighlyuxdoffensiveuxdtermuxdAnduxdTrumpuxdinsinuateduxdthatuxdSenuxdKamalauxdHarrisuxdwhosuxdBlackuxddoesntuxdmeetuxdtheuxdrequirementsuxdtouxdrunuxdforuxdviceuxdpresidentuxduxdauxdrepeatuxdofuxdtheuxdbirtheruxdconspiracyuxdtheoryuxdthatuxdheuxdperpetuateduxdaboutuxdformeruxdPresidentuxdBarackuxdObama\n"
     ]
    }
   ],
   "source": [
    "def format_raw_text(token):\n",
    "        token = token.replace(' ', 'uxd')\n",
    "        clean_token = ''.join(chr for chr in token if chr.isalnum() and chr.isalpha())\n",
    "        return clean_token\n",
    "\n",
    "inp_basis = 'Most recently, Trump has called the SARS-CoV-2 coronavirus the “Chinese virus” and “kung flu” — racist terms that tap into the kind of xenophobia that he latched onto during his 2016 presidential campaign; Trump’s own adviser, Kellyanne Conway, previously called “kung flu” a “highly offensive” term. And Trump insinuated that Sen. Kamala Harris, who’s Black, “doesn’t meet the requirements” to run for vice president — a repeat of the birther conspiracy theory that he perpetuated about former President Barack Obama.'\n",
    "print(format_raw_text(inp_basis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8950\n",
      "2238\n",
      "(2,)\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_asarray.py:102: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "primary_data = clean_truth_data #secondary option of truth_data\n",
    "\n",
    "train_len = int(len(primary_data)*0.8)\n",
    "test_len = len(primary_data) - train_len\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(primary_data, [train_len, test_len])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "\n",
    "num_feats = np.array([train_set[i][0].numpy() for i in range(len(train_set))])\n",
    "num_labels = np.array([train_set[i][1] for i in range(len(train_set))])\n",
    "\n",
    "a = iter(train_loader)\n",
    "b = next(a)\n",
    "b = np.asarray(b)\n",
    "print(b.shape)\n",
    "inp_size = (b[0].shape)[1]\n",
    "print(inp_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import itertools\n",
    "# ab = list(itertools.chain(*[i[0] for i in clean_truth_data]))\n",
    "# print(len(ab))\n",
    "# ab = set([int(i) for i in ab])\n",
    "# emb_dim = len(ab)\n",
    "\n",
    "emb_dim = 6712800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForward(\n",
      "  (fc1): Linear(in_features=600, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (fc4): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc5): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc6): Linear(in_features=50, out_features=20, bias=True)\n",
      "  (fc7): Linear(in_features=20, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ") RecurrentClassifier(\n",
      "  (embedding): Embedding(6712800, 600)\n",
      "  (rnn): LSTM(600, 50, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (fc1): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, kernel_size=4):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 200)\n",
    "        self.fc3 = nn.Linear(200, 100)        \n",
    "        self.fc4 = nn.Linear(100, 100)\n",
    "        self.fc5 = nn.Linear(100, 50)\n",
    "        self.fc6 = nn.Linear(50, 20)\n",
    "        self.fc7 = nn.Linear(20, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.dropout(F.relu(self.fc4(x)))\n",
    "        x = self.dropout(F.relu(self.fc5(x)))\n",
    "        x = self.dropout(F.relu(self.fc6(x)))\n",
    "        x = self.dropout(F.relu(self.fc7(x)))\n",
    "\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class RecurrentClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, input_size, hidden_size, output_size, num_layers, dropout=0.3):\n",
    "        super(RecurrentClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(embedding_dim, input_size)\n",
    "        self.rnn = nn.LSTM(input_size, \n",
    "                            hidden_size,\n",
    "                            num_layers,\n",
    "                            batch_first = True,\n",
    "                            dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, (hidden, cell) = self.rnn(x)\n",
    "        print(hidden.shape)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1, :, :]), dim=1))\n",
    "        x = self.fc1(hidden)\n",
    "        x = self.dropout(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "max_len = len(train_set[1][0])\n",
    "ref_check = 1\n",
    "\n",
    "feedforward = FeedForward(ref_check, inp_size).to(device)\n",
    "recurrent = RecurrentClassifier(emb_dim, inp_size, 50, ref_check, 2, dropout=0.2).to(device)\n",
    "\n",
    "print(feedforward, recurrent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(net, train_loader, LR, DECAY, EPOCHS):\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LR, weight_decay=DECAY)\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    epochs = EPOCHS\n",
    "    losses = []\n",
    "\n",
    "    for step in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inp, labels = data\n",
    "            if net == recurrent:\n",
    "                inp, labels = inp.long().to(device), labels.float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inp)\n",
    "                cost = loss_func(torch.squeeze(outputs), torch.squeeze(labels))\n",
    "            elif net == feedforward:\n",
    "                inp, labels = inp.float().to(device), labels.float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inp)\n",
    "                cost = loss_func(torch.squeeze(outputs), labels)\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += cost.item()\n",
    "        print(f'Epoch: {step}   Training Loss: {running_loss/len(train_loader)}')\n",
    "    print('Training Complete')  \n",
    "\n",
    "    return losses\n",
    "\n",
    "def eval(net, test_loader):\n",
    "    total = 0\n",
    "    acc = 0\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LR, weight_decay=DECAY)\n",
    "\n",
    "    for i, data in enumerate(test_loader):\n",
    "        inp, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        output = net(inp.float())\n",
    "        output = output.detach().numpy()\n",
    "        output = list(output)\n",
    "        output = [list(i).index(max(i)) for i in output]\n",
    "        \n",
    "        for idx, item in enumerate(torch.tensor(output)):\n",
    "            total += 1\n",
    "            if item == labels[idx]:\n",
    "                acc += 1\n",
    "    print(f'{acc/total*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_parameters/lstm_politifact.pth'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_load(net, PATH, name, export=True):\n",
    "    if export:\n",
    "        torch.save(net.state_dict(), PATH+name+'.pth')\n",
    "        return PATH+name+'.pth'\n",
    "    else:\n",
    "        net.torch.load_state_dict(torch.load(PATH + name + '.pth'))\n",
    "        return net\n",
    "    \n",
    "# train(feedforward, train_loader, 1e-3, 5e-3, 200)\n",
    "model_load(feedforward, 'model_parameters/', 'linear_politifact')\n",
    "\n",
    "# train(recurrent, train_loader, 1e-3, 5e-3, 200)\n",
    "model_load(recurrent, 'model_parameters/', 'lstm_politifact')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 600])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = 'https://www.historyofvaccines.org/content/blog/defense-of-common-antivaxxer'\n",
    "\n",
    "_, _, _, _, _, summ = meta_extract(url)\n",
    "\n",
    "token_basis = clean_truth_data.token\n",
    "\n",
    "\n",
    "def tokenize_sequence(text_inp, tokenizer):\n",
    "    text_inp = text_inp.lower().split('\\n')\n",
    "    tokenizer.fit_on_texts(text_inp)\n",
    "    sequences = tokenizer.texts_to_sequences(summ)\n",
    "    sequences = [i if i!=[] else [0] for i in tokenizer.texts_to_sequences(summ)]\n",
    "    sequences = [i[0] for i in sequences]\n",
    "    pad_len =  [0]*int(inp_size - len(sequences))\n",
    "    sequences += pad_len\n",
    "    return torch.FloatTensor(sequences)[:600]\n",
    "\n",
    "inp = tokenize_sequence(summ, token_basis)\n",
    "inp = inp[None, :]\n",
    "print(inp.shape)\n",
    "\n",
    "def prediction(inp, model):\n",
    "    output = model(inp)\n",
    "    return output\n",
    "\n",
    "a = prediction(inp, feedforward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# feedforward_template = FeedForward(ref_check, inp_size).to(device)\n",
    "# recurrent_template = RecurrentClassifier(emb_dim, inp_size, 50, ref_check, 2, dropout=0.2).to(device)    \n",
    "# model_load(feedforward_template, '/Users/devpatelio/Downloads/Coding/Global_Politics_EA/model_parameters/', 'linear_politifact')\n",
    "# model_load(recurrent_template, '/Users/devpatelio/Downloads/Coding/Global_Politics_EA/model_parameters/', 'lstm_politifact')\n",
    "\n",
    "# # feedforward_template.eval()\n",
    "# # recurrent_template.eval()\n",
    "\n",
    "# output_linear = '0'\n",
    "# output_lstm = '1' #check for error without passing error\n",
    "\n",
    "# output_linear = prediction(inp, feedforward_template)\n",
    "# output_lstm = prediction(inp.long(), recurrent_template)\n",
    "\n",
    "# if output_linear == 0:\n",
    "#     output_linear = f\"No Bias: Prediction = {output_linear}\"\n",
    "# elif output_linear == 1:\n",
    "#     output_linear = f\"Bias: Prediction = {output_linear}\"\n",
    "\n",
    "\n",
    "# if output_lstm == 0:\n",
    "#     output_lstm = f\"Limited Veracity: Prediction = {output_lstm}\"\n",
    "# elif output_lstm == 1:\n",
    "#     output_lstm = f\"Expressive Veracity: Prediction = {output_lstm}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py\", line 1262, in run\n",
      "    self.function(*self.args, **self.kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/flask_ngrok.py\", line 70, in start_ngrok\n",
      "    ngrok_address = _run_ngrok()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/flask_ngrok.py\", line 31, in _run_ngrok\n",
      "    ngrok = subprocess.Popen([executable, 'http', '5000'])\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py\", line 947, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py\", line 1819, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "PermissionError: [Errno 13] Permission denied: '/var/folders/q6/c9ls7y117c383dxbpmvcj56h0000gn/T/ngrok/ngrok'\n",
      "127.0.0.1 - - [25/Feb/2022 12:58:21] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [25/Feb/2022 12:58:24] \"\u001b[37mGET /text HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [25/Feb/2022 12:58:26] \"\u001b[32mPOST /text HTTP/1.1\u001b[0m\" 302 -\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from django.shortcuts import render\n",
    "# from scrape import meta_extract\n",
    "import flask\n",
    "from flask import Flask, request, render_template, redirect, url_for\n",
    "# from sentiment import sentiment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import html\n",
    "import torch\n",
    "# from linear_politifact_basis import token_basis, convert_word_to_token\n",
    "from flask_ngrok import run_with_ngrok\n",
    "from flask import Flask, render_template, redirect, url_for, request\n",
    "app = Flask(__name__)\n",
    "run_with_ngrok(app)   #starts ngrok when the app is run\n",
    "\n",
    "from flask import Flask, jsonify, request\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def model_load(net, PATH, name, export=True):\n",
    "    if export:\n",
    "        torch.save(net.state_dict(), PATH+name+'.pth')\n",
    "        return PATH+name+'.pth'\n",
    "    else:\n",
    "        net.torch.load_state_dict(torch.load(PATH + name + '.pth'))\n",
    "        net.eval()\n",
    "\n",
    "@app.route('/')\n",
    "def hello():\n",
    "    return render_template(\"home.html\")\n",
    "\n",
    "@app.route('/link', methods=[\"POST\", \"GET\"])\n",
    "def link():\n",
    "    if request.method == \"POST\":\n",
    "        link_inp = request.form['linker']\n",
    "        # print(type(link_inp))\n",
    "    \n",
    "        link_inp = link_inp.replace('.com', 'comkey')\n",
    "        link_inp = link_inp.replace('https://', 'https')\n",
    "        link_inp = link_inp.replace('www.', 'www')\n",
    "        link_inp = link_inp.replace('/', 'slash')\n",
    "        # print(link_inp)\n",
    "        main = link_inp\n",
    "\n",
    "        return redirect(url_for(\"preview_linker\", linkage=main, tag='link_url'))\n",
    "    else:\n",
    "        return render_template(\"link.html\")\n",
    "\n",
    "\n",
    "@app.route('/text', methods=[\"POST\", \"GET\"])\n",
    "def pure_text():\n",
    "    if request.method == \"POST\":\n",
    "        inp_raw = request.form['raw_text']\n",
    "        inp_raw = format_raw_text(inp_raw)\n",
    "        return redirect(url_for('preview_linker', linkage=inp_raw, tag='pure_text'))\n",
    "    else:\n",
    "        return render_template(\"pure_text.html\")\n",
    "\n",
    "\n",
    "@app.route(f\"/output/<tag>/<linkage>\")\n",
    "def preview_linker(linkage, tag):\n",
    "    preview = linkage\n",
    "    if tag == 'link_url':\n",
    "        preview = preview.replace('https', 'https://')\n",
    "        preview = preview.replace('www', 'www.')\n",
    "        preview = preview.replace('slash', '/')\n",
    "        preview = preview.replace('comkey', '.com')\n",
    "        authart, publ, timg, allimg, tit, summ = meta_extract(preview)\n",
    "\n",
    "    elif tag == 'pure_text':\n",
    "        preview = preview.replace('uxd', ' ')     \n",
    "        summ = preview  \n",
    "        empty_msg = 'None'\n",
    "        authart = empty_msg\n",
    "        publ = empty_msg\n",
    "        timg = empty_msg\n",
    "        allimg = empty_msg\n",
    "        tit = empty_msg\n",
    "\n",
    "    sent = sentiment(summ)\n",
    "\n",
    "    inp = tokenize_sequence(summ, token_basis)\n",
    "    inp = inp[:600]\n",
    "    inp = inp[None, :]\n",
    "    # print(inp.shape)\n",
    "\n",
    "    feedforward_template = FeedForward(ref_check, inp_size).to(device)\n",
    "    recurrent_template = RecurrentClassifier(emb_dim, inp_size, 50, ref_check, 2, dropout=0.2).to(device)    \n",
    "    model_load(feedforward_template, 'model_parameters/', 'linear_politifact')\n",
    "    model_load(recurrent_template, 'model_parameters/', 'lstm_politifact')\n",
    "\n",
    "    # feedforward_template.eval()\n",
    "    # recurrent_template.eval()\n",
    "\n",
    "    output_linear = '0 ERROR'\n",
    "    output_lstm = '1 ERROR' #check for error without passing error\n",
    "\n",
    "    output_linear = F.sigmoid(prediction(inp, feedforward_template)).round()\n",
    "    output_lstm = F.sigmoid(prediction(inp.long(), recurrent_template))\n",
    "\n",
    "    all_types = list(pd.read_csv(data_dict['politifact_clean'])['veracity'].unique())\n",
    "\n",
    "    if output_linear == 0:\n",
    "        output_linear = f\"Little Bias: Prediction = {output_linear}\"\n",
    "    elif output_linear == 1:\n",
    "        output_linear = f\"Substantial Bias: Prediction = {output_linear}\"\n",
    "\n",
    "    statement_type = ''\n",
    "    if output_lstm <= 0.25:\n",
    "        statement_type = 'True'\n",
    "    elif 0.25 < output_lstm <= 0.5:\n",
    "        statement_type = 'Mostly True'\n",
    "    elif 0.5 < output_lstm <= 0.75:\n",
    "        statement_type = 'Mostly False'\n",
    "    elif 0.75 < output_lstm <= 1:\n",
    "        statement_type = 'False'\n",
    "    elif output_lstm > 1:\n",
    "        statement_type = 'Pants on Fire!'\n",
    "\n",
    "    output_lstm = f\"Veracity -> {statement_type}: {output_lstm}\"\n",
    "\n",
    "    # if output_lstm == 0:\n",
    "    #     output_lstm = f\"Limited Veracity: Prediction = {output_lstm}\"\n",
    "    # elif output_lstm == 1:\n",
    "    #     output_lstm = f\"Expressive Veracity: Prediction = {output_lstm}\"\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    return render_template(\"preview.html\", preview_link=preview,\n",
    "                                            author_article=authart, \n",
    "                                            published_article=publ,\n",
    "                                            top_image = timg,\n",
    "                                            all_image = allimg,\n",
    "                                            title_article=tit,\n",
    "                                            summary_article=summ,\n",
    "                                            sentiment=sent,\n",
    "                                            bias_point=output_linear,\n",
    "                                            skew_point=output_lstm)\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     app.run(host='0.0.0.0')\n",
    "\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch=1.10.2\n",
      "pandas=1.1.4\n",
      "numpy=1.20.3\n",
      "newspaper=0.2.8\n",
      "tensorflow=2.8.0\n",
      "keras=2.8.0\n"
     ]
    }
   ],
   "source": [
    "import newspaper\n",
    "import tensorflow\n",
    "\n",
    "print('torch='+torch.__version__)\n",
    "print('pandas='+pd.__version__)\n",
    "print('numpy='+np.__version__)\n",
    "\n",
    "print('newspaper='+newspaper.__version__)\n",
    "print('tensorflow='+tensorflow.__version__)\n",
    "print('keras='+keras.__version__)\n",
    "\n",
    "\n",
    "\n",
    "# print(nltk.__spec__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
