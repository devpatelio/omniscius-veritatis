{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-bdf3e9850cd3>:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  transformed_data[x][idx] = all_words\n",
      "<ipython-input-20-bdf3e9850cd3>:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bag_dataset[row][idx] = vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-bdf3e9850cd3>:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column][idx] = torch.tensor(vectorized, dtype=float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([    0.,  1292.,  9389.,  7024.,  2752.,  4790.,  6868.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0., 10438.,\n",
      "        4913.,  1993., 10051.,  7293.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.]), 0)\n"
     ]
    }
   ],
   "source": [
    "from data import data_dict, DIRECTORY\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import nltk\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "class BiasDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file, root, x_col, y_col, meta_columns, label_idx = -1):\n",
    "        self.data = pd.read_csv(file)\n",
    "        self.og_data = self.data\n",
    "        self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        self.tokenized_data, self.base_ref = self.tokenizer(self.data, [x_col])\n",
    "        self.clean_data, self.errors = self.word_vector(self.tokenized_data.drop(meta_columns, axis=1), self.base_ref, [x_col])\n",
    "        \n",
    "        self.data = self.vectorize(self.clean_data, [y_col])\n",
    "        self.data = self.padding(self.padding(self.data, [x_col]), [x_col])\n",
    "        self.data = self.data.to_numpy()\n",
    "\n",
    "        max_len = max([len(i[0]) for i in self.data])\n",
    "\n",
    "        for idx, ent_item in enumerate(self.data):\n",
    "            add_array = np.array([0]*(max_len-len(ent_item[0])))\n",
    "            self.data[idx][0] = np.concatenate((ent_item[0], add_array))\n",
    "\n",
    "        \n",
    "        self.root = root\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def transform(self, data, col_names):\n",
    "        for col in col_names:\n",
    "            uniques = [list(set(data[col].values))][0]\n",
    "            uniques = [x for x in uniques if str(x) != 'nan']\n",
    "            one_hot = np.identity(len(uniques))\n",
    "            one_hot = [str(i) for i in one_hot.tolist()]\n",
    "            one_dict = dict(zip(uniques, one_hot))\n",
    "            data = data.replace(one_dict)\n",
    "        return data\n",
    "\n",
    "    def format_text(self, token):\n",
    "        clean_token = ''.join(chr for chr in token if chr.isalnum() and chr.isalpha())\n",
    "        return clean_token\n",
    "\n",
    "    def tokenizer(self, data, text_cols, window=2):\n",
    "        all_text = set()\n",
    "        transformed_data = data\n",
    "        for x in text_cols:\n",
    "            for idx, entry in enumerate(data[x]):\n",
    "                clean_entry = list(map(self.format_text, (word for word in entry)))\n",
    "                append_all_text = set()\n",
    "                for y, char in enumerate(clean_entry):\n",
    "                    if char == '':\n",
    "                        clean_entry[y] = ' '\n",
    "                all_words = ''.join(i.lower() for i in clean_entry)\n",
    "                transformed_data[x][idx] = all_words\n",
    "                \n",
    "                for m in set(all_words.split(' ')):\n",
    "                    all_text.add(m)\n",
    "\n",
    "        return transformed_data, dict(zip(list(all_text), [z for z in range(len(all_text))]))\n",
    "\n",
    "    def word_vector(self, data, ref, text_cols):\n",
    "        bag_dataset = data\n",
    "        errors = []\n",
    "        for row in text_cols:\n",
    "            for idx, entry in enumerate(data[row]):\n",
    "                list_entry = entry.split(' ')\n",
    "                try:\n",
    "                    vector = torch.FloatTensor([ref[word.lower()] for word in list_entry])\n",
    "                except:\n",
    "                    errors.append([list_entry, idx])\n",
    "                bag_dataset[row][idx] = vector\n",
    "        \n",
    "        return bag_dataset, errors\n",
    "\n",
    "    def vectorize(self, data_inp, columns):\n",
    "        data = data_inp\n",
    "        for column in columns:\n",
    "            labels = list(data[column].unique())\n",
    "            ref = dict(zip(data[column].unique(), [i for i in range(len(labels))]))\n",
    "            print(ref)\n",
    "            for idx, val in enumerate(data[column]):\n",
    "                vectorized = ref[data[column][idx]]\n",
    "                data[column][idx] = torch.tensor(vectorized, dtype=float)\n",
    "        return data\n",
    "    \n",
    "    def padding(self, data, x_column):\n",
    "        deep_copy = data\n",
    "        max_len = max([len(i) for i in deep_copy[x_column]])\n",
    "        for idx, i in enumerate(deep_copy[x_column]):\n",
    "            if len(i) != max_len:\n",
    "                flag = False\n",
    "                print(max_len-len(i))\n",
    "                print(deep_copy[x_column][idx], torch.cat((deep_copy[x_column][idx], torch.FloatTensor([0]*(max_len-len(i))))))\n",
    "                deep_copy[x_column][idx] = torch.cat((deep_copy[x_column][idx], torch.FloatTensor([0]*(max_len-len(i)))))\n",
    "            else:\n",
    "                flag = True\n",
    "                pass\n",
    "        else:\n",
    "            return deep_copy\n",
    "\n",
    "    \n",
    "    def __len__ (self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__ (self, idx):\n",
    "        self.transpose_data = self.data\n",
    "        self.transpose_data = self.transpose_data.transpose()\n",
    "        x_data = self.transpose_data[0]\n",
    "        y_data = self.transpose_data[1]\n",
    "\n",
    "        return x_data[idx], y_data[idx]\n",
    "    \n",
    "\n",
    "truth_data = BiasDataset(data_dict['politifact_clean_binarized'], DIRECTORY, 'statement', 'veracity', ['source', 'link'])\n",
    "print(truth_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Roy Barnes is part of the team that has run up trillions of dollars in debt that Americans will spend years paying off, and they've done it without creating one job.\"\n",
      "167\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   9\n",
      " 453   3   1  10  13 418  76   3 155   2 131  10 110  47 357  34 484 240\n",
      "   6 479  23 252  44 163]\n",
      "{0: 0, 1: 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-f60de35552d6>:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column][idx] = torch.tensor(vectorized, dtype=float)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class PreprocessingDataset(Dataset):\n",
    "    def __init__(self, file, root, x_col, y_col, meta_columns, label_idx = -1):\n",
    "        self.x_col = x_col\n",
    "        self.y_col = y_col\n",
    "        self.data = pd.read_csv(file)\n",
    "        self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "        self.data = self.data.drop(meta_columns, axis=1)\n",
    "\n",
    "        # self.data, self.base_ref = self.tokenizer(self.data, [x_col])\n",
    "        self.x_data = self.data[x_col]\n",
    "        self.max_len = max([len(i) for i in self.x_data])\n",
    "        self.max_len = 600\n",
    "\n",
    "        self.x_data = self.word_vector(self.x_data)\n",
    "        self.data[x_col] = [torch.FloatTensor(i) for i in self.x_data]\n",
    "        self.data = self.vectorize(self.data, [y_col])\n",
    "        self.df_data = self.data\n",
    "        self.data = self.data.to_numpy()\n",
    "\n",
    "        self.root = root\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def format_text(self, token):\n",
    "        clean_token = ''.join(chr for chr in token if chr.isalnum() and chr.isalpha())\n",
    "        return clean_token\n",
    "\n",
    "    def word_vector(self, data):\n",
    "        x_data = data\n",
    "        x_data = list(x_data)\n",
    "        maximum_length = 0\n",
    "        max_idx = 0\n",
    "        for idx, i in enumerate(x_data):\n",
    "\n",
    "            if len(i) > maximum_length:\n",
    "                maximum_length = len(i)\n",
    "                max_idx = idx\n",
    "        maximum_length = 600\n",
    "        t = Tokenizer(num_words=600)\n",
    "        t.fit_on_texts(x_data)\n",
    "        sequences = t.texts_to_sequences(x_data)\n",
    "        sequences = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=maximum_length)\n",
    "        print(x_data[0])\n",
    "        print(len(x_data[0]))\n",
    "        print(sequences[0])\n",
    "\n",
    "        return sequences\n",
    "\n",
    "\n",
    "    def vectorize(self, data_inp, columns):\n",
    "        data = data_inp\n",
    "        for column in columns:\n",
    "            labels = list(data[column].unique())\n",
    "            ref = dict(zip(data[column].unique(), [i for i in range(len(labels))]))\n",
    "            print(ref)\n",
    "            for idx, val in enumerate(data[column]):\n",
    "                vectorized = ref[data[column][idx]]\n",
    "                data[column][idx] = torch.tensor(vectorized, dtype=float)\n",
    "        return data\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__ (self, idx):\n",
    "        \n",
    "        self.transpose_data = self.data\n",
    "        self.transpose_data = self.transpose_data.transpose()\n",
    "        x_data = self.transpose_data[0]\n",
    "        y_data = self.transpose_data[1]\n",
    "\n",
    "        return x_data[idx], y_data[idx]\n",
    "\n",
    "clean_truth_data = PreprocessingDataset(data_dict['politifact_clean_binarized'], DIRECTORY, 'statement', 'veracity', ['source', 'link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>veracity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement  veracity\n",
       "0  [tensor(0.), tensor(0.), tensor(0.), tensor(0....         0\n",
       "1  [tensor(0.), tensor(0.), tensor(0.), tensor(0....         0\n",
       "2  [tensor(0.), tensor(0.), tensor(0.), tensor(0....         0\n",
       "3  [tensor(0.), tensor(0.), tensor(0.), tensor(0....         0\n",
       "4  [tensor(0.), tensor(0.), tensor(0.), tensor(0....         0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(clean_truth_data[0])\n",
    "clean_truth_data.df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-4fe4853537fe>:18: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  num_feats = np.array([train_set[i][0]for i in range(len(train_set))])\n",
      "<ipython-input-36-4fe4853537fe>:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  num_feats = np.array([train_set[i][0]for i in range(len(train_set))])\n",
      "<ipython-input-36-4fe4853537fe>:27: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  b = np.array(next(a))\n",
      "<ipython-input-36-4fe4853537fe>:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  b = np.array(next(a))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "primary_data = clean_truth_data #secondary option of truth_data\n",
    "\n",
    "train_len = int(len(primary_data)*0.8)\n",
    "test_len = len(primary_data) - train_len\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(primary_data, [train_len, test_len])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# print(len(train_set))\n",
    "# print(len(test_set))\n",
    "# print(len(primary_data))\n",
    "\n",
    "num_feats = np.array([train_set[i][0]for i in range(len(train_set))])\n",
    "num_labels = np.array([train_set[i][1]for i in range(len(train_set))])\n",
    "\n",
    "\n",
    "# print(num_feats.shape)\n",
    "# print(num_labels.shape)\n",
    "\n",
    "if primary_data == clean_truth_data:\n",
    "    a = iter(train_loader)\n",
    "    b = np.array(next(a))\n",
    "    inp_size = (b[0].shape)[1]\n",
    "else:\n",
    "    inp_size = str(num_feats[0].shape)[-5:-2]\n",
    "\n",
    "print(inp_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "FeedForward(\n",
      "  (fc1): Linear(in_features=600, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (fc4): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc5): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc6): Linear(in_features=50, out_features=20, bias=True)\n",
      "  (fc7): Linear(in_features=20, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, kernel_size=4):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 200)\n",
    "        self.fc3 = nn.Linear(200, 100)        \n",
    "        self.fc4 = nn.Linear(100, 100)\n",
    "        self.fc5 = nn.Linear(100, 50)\n",
    "        self.fc6 = nn.Linear(50, 20)\n",
    "        self.fc7 = nn.Linear(20, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.dropout(F.relu(self.fc4(x)))\n",
    "        x = self.dropout(F.relu(self.fc5(x)))\n",
    "        x = self.dropout(F.relu(self.fc6(x)))\n",
    "        x = self.dropout(F.relu(self.fc7(x)))\n",
    "\n",
    "        \n",
    "        return x\n",
    "\n",
    "class RecurrentClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout=0.3):\n",
    "        super(RecurrentClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, \n",
    "                            hidden_size,\n",
    "                            num_layers,\n",
    "                            dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, (hidden, cell) = self.rnn(x)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1, :, :]), dim=1))\n",
    "        x = self.fc1(hidden)\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        x = torch.squeeze(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "max_len = len(train_set[1][0])\n",
    "ref_check = 1\n",
    "print(max_len)\n",
    "\n",
    "# net = RecurrentClassifier(int(inp_size), 256, ref_check, 3, dropout=0.2)\n",
    "net = FeedForward(ref_check, inp_size)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR = 1e-3\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR, weight_decay=5e-3)\n",
    "loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "epochs = 500\n",
    "losses = []\n",
    "\n",
    "for step in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inp, labels = data\n",
    "        inp, labels = inp.float(), labels.float()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inp)\n",
    "        cost = loss_func(torch.squeeze(outputs), labels)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += cost.item()\n",
    "    print(f'Epoch: {step}   Training Loss: {running_loss/len(train_loader)}')\n",
    "print('Training Complete')  \n",
    "\n",
    "\n",
    "\n",
    "# class CNNTextClassifier(nn.Module):\n",
    "#     def __init__ (self, oshape, ishape):\n",
    "#         super(CNNTextClassifier, self).__init__() \n",
    "#         self.conv1 = nn.Conv1d(ishape, 150)\n",
    "#         self.maxpool1 = nn.MaxPool1d(150)\n",
    "#         self.conv2 = nn.Conv1d(150, 100)\n",
    "#         self.maxpool2 = nn.MaxPool1d(100)\n",
    "#         self.conv3 = nn.Conv1d(100, 50)\n",
    "#         self.maxpool3 = nn.MaxPool1d(50)\n",
    "#         self.fc1 = nn.Linear(50, 25)\n",
    "#         self.fc2 = nn.Linear(25, oshape)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.maxpool1(F.relu(self.conv1(x)))\n",
    "#         x = self.maxpool2(F.relu(self.conv2(x)))\n",
    "#         x = self.maxpool3(F.relu(self.conv3(x)))\n",
    "#         x = x.flatten()\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.25558534405719%\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "acc = 0\n",
    "\n",
    "for i, data in enumerate(test_loader):\n",
    "    inp, labels = data\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = net(inp.float())\n",
    "    output = output.detach().numpy()\n",
    "    output = list(output)\n",
    "    output = [list(i).index(max(i)) for i in output]\n",
    "    \n",
    "    for idx, item in enumerate(torch.tensor(output)):\n",
    "        total += 1\n",
    "        if item == labels[idx]:\n",
    "            acc += 1\n",
    "print(f'{acc/total*100}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'model_parameters/linear_politifact.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def format_text(token):\n",
    "#     clean_token = ''.join(chr for chr in token if chr.isalnum() and chr.isalpha())\n",
    "#     return clean_token\n",
    "\n",
    "\n",
    "# def tokenizer(data, text_cols, window=2):\n",
    "#     all_text = set()\n",
    "#     transformed_data = data\n",
    "#     for x in text_cols:\n",
    "#         for idx, entry in enumerate(data[x]):\n",
    "#             clean_entry = list(map(format_text, (word for word in entry)))\n",
    "#             append_all_text = set()\n",
    "#             for y, char in enumerate(clean_entry):\n",
    "#                 if char == '':\n",
    "#                     clean_entry[y] = ' '\n",
    "#             all_words = ''.join(i.lower() for i in clean_entry)\n",
    "#             transformed_data[x][idx] = all_words\n",
    "            \n",
    "#             for m in set(all_words.split(' ')):\n",
    "#                 all_text.add(m)\n",
    "\n",
    "#     return transformed_data, dict(zip(list(all_text), [z for z in range(len(all_text))]))\n",
    "            \n",
    "# truth_data = BiasDataset(data_dict['politifact'], DIRECTORY).data\n",
    "# truth_data_clean = BiasDataset(data_dict['politifact_clean'], DIRECTORY).data\n",
    "# truth_data_binary = BiasDataset(data_dict['politifact_clean_binarized'], DIRECTORY).data\n",
    "\n",
    "\n",
    "# truth_preprocessed, base_ref = tokenizer(truth_data_clean, ['statement'])\n",
    "# truth_preprocessed.head()\n",
    "# ##Custom Vector Embeddings\n",
    "# def word_vector(data, ref, text_cols):\n",
    "#     bag_dataset = data\n",
    "#     errors = []\n",
    "#     for row in text_cols:\n",
    "#         for idx, entry in enumerate(data[row]):\n",
    "#             list_entry = entry.split(' ')\n",
    "#             try:\n",
    "#                 vector = torch.FloatTensor([ref[word.lower()] for word in list_entry])\n",
    "#             except:\n",
    "#                 errors.append([list_entry, idx])\n",
    "#             bag_dataset[row][idx] = vector\n",
    "    \n",
    "#     return bag_dataset, errors\n",
    "\n",
    "\n",
    "\n",
    "# truth_data, errors = word_vector(truth_preprocessed.drop(['source', 'link'], axis=1), base_ref, ['statement'])\n",
    "# truth_data.head()\n",
    "\n",
    "# labels_check = list(truth_data['veracity'].unique())\n",
    "# ref_check = dict(zip(labels_check, [i for i in range(len(labels_check))]))\n",
    "# print(ref_check)\n",
    "\n",
    "# def vectorize(data_inp, columns):\n",
    "#     data = data_inp\n",
    "#     for column in columns:\n",
    "#         labels = list(truth_data[column].unique())\n",
    "#         ref = dict(zip(data[column].unique(), [i for i in range(len(labels))]))\n",
    "#         print(ref)\n",
    "#         for idx, val in enumerate(data[column]):\n",
    "#             vectorized = ref[data[column][idx]]\n",
    "#             data[column][idx] = torch.tensor(vectorized, dtype=float)\n",
    "#     return data\n",
    "\n",
    "# truth_processed = vectorize(truth_data, ['veracity'])\n",
    "\n",
    "# truth_processed.head()\n",
    "\n",
    "# truth_vector = truth_processed\n",
    "\n",
    "# max_len = max([len(i) for i in truth_vector['statement']])\n",
    "# for idx, i in enumerate(truth_vector['statement']):\n",
    "#     if len(i) != max_len:\n",
    "#         flag = False\n",
    "#         truth_vector['statement'][idx] = torch.cat((truth_vector['statement'][idx], torch.FloatTensor([0]*(max_len-len(i)))), 0)\n",
    "#         # print(len(truth_vector['statement'][idx]))\n",
    "#     else:\n",
    "#         flag = True\n",
    "\n",
    "# print(flag)\n",
    "# truth_vector.head() \n",
    "\n",
    "# processed_dataset = truth_vector\n",
    "\n",
    "# train_len = int(len(processed_dataset)*0.8)\n",
    "# test_len = len(processed_dataset) - train_len\n",
    "\n",
    "# train_dataset = processed_dataset.sample(n=train_len)\n",
    "# test_dataset = processed_dataset[~processed_dataset.isin(train_dataset)].dropna()\n",
    "# test_dataset.reset_index(inplace=True)\n",
    "# train_dataset.reset_index(inplace=True)\n",
    "\n",
    "# train_dataset = BiasDataset(train_dataset, DIRECTORY)\n",
    "# test_dataset = BiasDataset(test_dataset, DIRECTORY)\n",
    "\n",
    "# print(len(train_dataset))\n",
    "# print(len(test_dataset))\n",
    "# print(len(processed_dataset))\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
