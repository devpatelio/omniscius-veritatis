{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from data import data_dict, DIRECTORY\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import nltk\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_csv(data_dict['news_articles'])\n",
    "\n",
    "class BinarizedDataset(Dataset):\n",
    "    def __init__(self, directory, x_col, y_col, meta_col, drop_col, merge_x):\n",
    "        self.data = pd.read_csv(directory, keep_default_na=False)\n",
    "        self.x_col = x_col\n",
    "        self.y_col = y_col\n",
    "        self.meta_col = meta_col\n",
    "\n",
    "        self.text_data = self.data\n",
    "        self.text_data = self.text_data.drop(drop_col, axis=1).drop(meta_col, axis=1)\n",
    "        \n",
    "        \n",
    "\n",
    "        self.text_data[x_col] = [str(str(self.text_data[x_col][idx]) + ' ' + str(self.text_data[merge_x][idx])) for idx in range(len(self.text_data))]\n",
    "        self.x_text_data = self.text_data[x_col]\n",
    "        self.max_len = max([len(i) for i in self.x_text_data])\n",
    "\n",
    "        # tool = language_tool_python.LanguageTool('en-US')\n",
    "        # all_x_data = self.x_text_data.tolist()\n",
    "        # all_x_data = '*$)#'.join(all_x_data)\n",
    "        # all_x_data = tool.correct(all_x_data)\n",
    "\n",
    "        # self.x_text_data = all_x_data.split('*$)#')\n",
    "\n",
    "        self.x_text_data = self.word_vector(self.x_text_data)\n",
    "        self.text_data[x_col] = [torch.LongTensor(i) for i in self.x_text_data]\n",
    "        self.text_data = self.vectorize(self.text_data, [y_col])\n",
    "\n",
    "        drop_idx = []\n",
    "        for idx, entry in enumerate(self.text_data['label']):\n",
    "            if entry == torch.tensor(2):\n",
    "                drop_idx.append(idx)\n",
    "        \n",
    "        self.text_data = self.text_data.drop(df.index[[drop_idx]])\n",
    "        self.text_data = self.text_data.drop([merge_x], axis=1)\n",
    "        \n",
    "        self.text_data = self.text_data[[x_col, y_col]]\n",
    "        self.df_data = self.text_data\n",
    "        self.data = self.text_data\n",
    "\n",
    "        self.data = self.data.to_numpy()\n",
    "    \n",
    "    def format_text(self, token):\n",
    "        clean_token = ''.join(chr for chr in token if chr.isalnum() and chr.isalpha())\n",
    "        return clean_token\n",
    "\n",
    "    def word_vector(self, data):\n",
    "        x_data = data\n",
    "        x_data = list(x_data)\n",
    "        maximum_length = 0\n",
    "        max_idx = 0\n",
    "        for idx, i in enumerate(x_data):\n",
    "\n",
    "            if len(i) > maximum_length:\n",
    "                maximum_length = len(i)\n",
    "                max_idx = idx\n",
    "        \n",
    "        t = Tokenizer()\n",
    "        t.fit_on_texts(x_data)\n",
    "        sequences = t.texts_to_sequences(x_data)\n",
    "        sequences = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=maximum_length)\n",
    "        print(x_data[0])\n",
    "        print(len(x_data[0]))\n",
    "        print(sequences[0])\n",
    "\n",
    "        return sequences\n",
    "    \n",
    "    def vectorize(self, data_inp, columns):\n",
    "        data = data_inp\n",
    "        for column in columns:\n",
    "            labels = list(data[column].unique())\n",
    "            ref = dict(zip(data[column].unique(), [i for i in range(len(labels))]))\n",
    "            print(ref)\n",
    "            for idx, val in enumerate(data[column]):\n",
    "                vectorized = ref[data[column][idx]]\n",
    "                data[column][idx] = torch.tensor(vectorized, dtype=float)\n",
    "        return data\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__ (self, idx):\n",
    "        \n",
    "        self.transpose_data = self.data\n",
    "        self.transpose_data = self.transpose_data.transpose()\n",
    "        x_data = self.transpose_data[0]\n",
    "        y_data = self.transpose_data[1]\n",
    "\n",
    "        return x_data[idx], y_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hate', nan, 'fake', 'bias', 'state', 'junksci', 'bs', 'conspiracy', 'satire'}\n",
      "{nan, 'Real', 'Fake'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'print pay back money plus interest entire family everyone came need deported asap take two years bust go another group stealing government taxpayers group somalis stole four million government benefits months weve reported numerous cases like one muslim refugeesimmigrants commit fraud scamming systemits way control related'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(set(list(df['type'])))\n",
    "print(set(list(df['label'])))\n",
    "\n",
    "meta_columns = ['author', 'published', 'site_url']\n",
    "drop_columns = ['main_img_url', 'language', 'title', 'text', 'hasImage', 'type']\n",
    "\n",
    "df.iloc[0]['text_without_stopwords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print pay back money plus interest entire family everyone came need deported asap take two years bust go another group stealing government taxpayers group somalis stole four million government benefits months weve reported numerous cases like one muslim refugeesimmigrants commit fraud scamming systemits way control related muslims busted stole millions govt benefits\n",
      "368\n",
      "[   0    0    0 ...  371 5068 1104]\n",
      "{'Real': 0, 'Fake': 1, '': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py:4111: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  result = getitem(key)\n"
     ]
    }
   ],
   "source": [
    "clean_media_dataset = BinarizedDataset(data_dict['news_articles'], 'text_without_stopwords', 'label', drop_columns, meta_columns, 'title_without_stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noice\n"
     ]
    }
   ],
   "source": [
    "check = clean_media_dataset\n",
    "a = list(check.df_data['label'])\n",
    "if torch.tensor(2) in a:\n",
    "    print(\"uh oh\")\n",
    "else:\n",
    "    print('noice')\n",
    "\n",
    "check.df_data.head()\n",
    "\n",
    "\n",
    "# self.text_data[x_col] = [str(self.text_data[x_col][idx]+self.text_data[merge_x][idx]) for idx in range(len(self.text_data))]\n",
    "copy_df = df\n",
    "copy_df['text_without_stopwords'] = [str(copy_df['title_without_stopwords'][idx]) + ' ' + str(copy_df['text_without_stopwords'][idx]) for idx in range(len(copy_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1676\n",
      "419\n",
      "(2,)\n",
      "22661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-3665d531dd3a>:16: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  num_feats = np.array([train_set[i][0]for i in range(len(train_set))])\n",
      "<ipython-input-6-3665d531dd3a>:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  num_feats = np.array([train_set[i][0]for i in range(len(train_set))])\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_asarray.py:102: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "primary_data = clean_media_dataset #secondary option of truth_data\n",
    "\n",
    "train_len = int(len(primary_data)*0.8)\n",
    "test_len = len(primary_data) - train_len\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(primary_data, [train_len, test_len])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "\n",
    "num_feats = np.array([train_set[i][0]for i in range(len(train_set))])\n",
    "num_labels = np.array([train_set[i][1]for i in range(len(train_set))])\n",
    "\n",
    "a = iter(train_loader)\n",
    "b = next(a)\n",
    "b = np.asarray(b)\n",
    "print(b.shape)\n",
    "inp_size = (b[0].shape)[1]\n",
    "print(inp_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47268\n"
     ]
    }
   ],
   "source": [
    "# import itertools\n",
    "# ab = list(itertools.chain(*[i[0] for i in clean_media_dataset]))\n",
    "# print(len(ab))\n",
    "# ab = set([int(i) for i in ab])\n",
    "# emb_dim = len(ab)\n",
    "\n",
    "all_x = [i[0] for i in clean_media_dataset]\n",
    "a = torch.cat(all_x)\n",
    "ab = torch.unique(a) \n",
    "emb_dim = len(ab)\n",
    "print(emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22661\n",
      "RecurrentClassifier(\n",
      "  (embedding): Embedding(47268, 22661)\n",
      "  (rnn): LSTM(22661, 50, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (fc1): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RecurrentClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, input_size, hidden_size, output_size, num_layers, dropout=0.3):\n",
    "        super(RecurrentClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(embedding_dim, input_size)\n",
    "        self.rnn = nn.LSTM(input_size, \n",
    "                            hidden_size,\n",
    "                            num_layers,\n",
    "                            batch_first = True,\n",
    "                            dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, (hidden, cell) = self.rnn(x)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1, :, :]), dim=1))\n",
    "        x = self.fc1(hidden)\n",
    "        x = self.dropout(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "max_len = len(train_set[1][0])\n",
    "ref_check = 2\n",
    "print(max_len)\n",
    "\n",
    "net = RecurrentClassifier(emb_dim, int(inp_size), 50, ref_check, 2, dropout=0.2)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR = 1e-3\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR, weight_decay=5e-3)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 1000\n",
    "losses = []\n",
    "\n",
    "for step in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inp, labels = data\n",
    "        inp, labels = inp.long(), labels.long()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inp)\n",
    "        cost = loss_func(outputs, labels)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += cost.item()\n",
    "    print(f'Epoch: {step}   Training Loss: {running_loss/len(train_loader)}')\n",
    "print('Training Complete')  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
