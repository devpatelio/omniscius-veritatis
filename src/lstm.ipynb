{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Ninety percent of people born in the 1940s ended up doing better financially than their parents. But those born in the 1980s, the much-maligned Millennials, have only a 50-50 chance of doing better (financially) than their parents, despite being the best-educated generation in our history.\"\n",
      "292\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0 1243   14    3   35  484    2    1 5498 1415   76  638  600\n",
      " 4466   16   45  822  104  191  484    2    1 2257    1  269 7512 2451\n",
      "   12   69    5  179  179 1483    3  638  600 4466   16   45  822 1051\n",
      "  246    1  581 3312 2072    2   31  185]\n",
      "{'True': 0, 'Mostly False': 1, 'Pants on Fire!': 2, 'Mostly True': 3, 'False': 4}\n"
     ]
    }
   ],
   "source": [
    "from data import data_dict, DIRECTORY\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import nltk\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class PreprocessingDataset(Dataset):\n",
    "    def __init__(self, file, root, x_col, y_col, meta_columns, label_idx = -1):\n",
    "        self.x_col = x_col\n",
    "        self.y_col = y_col\n",
    "        self.data = pd.read_csv(file)\n",
    "        self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "        self.data = self.data.drop(meta_columns, axis=1)\n",
    "\n",
    "        # self.data, self.base_ref = self.tokenizer(self.data, [x_col])\n",
    "        self.x_data = self.data[x_col]\n",
    "        self.max_len = max([len(i) for i in self.x_data])\n",
    "\n",
    "        self.x_data = self.word_vector(self.x_data)\n",
    "        self.data[x_col] = [torch.LongTensor(i) for i in self.x_data]\n",
    "        self.data = self.vectorize(self.data, [y_col])\n",
    "        self.df_data = self.data\n",
    "        self.data = self.data.to_numpy()\n",
    "\n",
    "        self.root = root\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def format_text(self, token):\n",
    "        clean_token = ''.join(chr for chr in token if chr.isalnum() and chr.isalpha())\n",
    "        return clean_token\n",
    "\n",
    "    def word_vector(self, data):\n",
    "        x_data = data\n",
    "        x_data = list(x_data)\n",
    "        maximum_length = 0\n",
    "        max_idx = 0\n",
    "        for idx, i in enumerate(x_data):\n",
    "\n",
    "            if len(i) > maximum_length:\n",
    "                maximum_length = len(i)\n",
    "                max_idx = idx\n",
    "        \n",
    "        t = Tokenizer()\n",
    "        t.fit_on_texts(x_data)\n",
    "        sequences = t.texts_to_sequences(x_data)\n",
    "        sequences = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=maximum_length)\n",
    "        print(x_data[0])\n",
    "        print(len(x_data[0]))\n",
    "        print(sequences[0])\n",
    "\n",
    "        return sequences\n",
    "\n",
    "\n",
    "    def vectorize(self, data_inp, columns):\n",
    "        data = data_inp\n",
    "        for column in columns:\n",
    "            labels = list(data[column].unique())\n",
    "            ref = dict(zip(data[column].unique(), [i for i in range(len(labels))]))\n",
    "            print(ref)\n",
    "            for idx, val in enumerate(data[column]):\n",
    "                vectorized = ref[data[column][idx]]\n",
    "                data[column][idx] = torch.tensor(vectorized, dtype=float)\n",
    "        return data\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__ (self, idx):\n",
    "        \n",
    "        self.transpose_data = self.data\n",
    "        self.transpose_data = self.transpose_data.transpose()\n",
    "        x_data = self.transpose_data[0]\n",
    "        y_data = self.transpose_data[1]\n",
    "\n",
    "        return x_data[idx], y_data[idx]\n",
    "\n",
    "clean_truth_data = PreprocessingDataset(data_dict['politifact_clean'], DIRECTORY, 'statement', 'veracity', ['source', 'link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>veracity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>tensor(0., dtype=torch.float64)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>tensor(1., dtype=torch.float64)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>tensor(2., dtype=torch.float64)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>tensor(0., dtype=torch.float64)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>tensor(3., dtype=torch.float64)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement  \\\n",
       "0  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "1  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "2  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "3  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "4  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "\n",
       "                          veracity  \n",
       "0  tensor(0., dtype=torch.float64)  \n",
       "1  tensor(1., dtype=torch.float64)  \n",
       "2  tensor(2., dtype=torch.float64)  \n",
       "3  tensor(0., dtype=torch.float64)  \n",
       "4  tensor(3., dtype=torch.float64)  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_truth_data.df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8950\n",
      "2238\n",
      "(2,)\n",
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-9e81bf2c5f3d>:16: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  num_feats = np.array([train_set[i][0]for i in range(len(train_set))])\n",
      "<ipython-input-8-9e81bf2c5f3d>:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  num_feats = np.array([train_set[i][0]for i in range(len(train_set))])\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_asarray.py:102: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "primary_data = clean_truth_data #secondary option of truth_data\n",
    "\n",
    "train_len = int(len(primary_data)*0.8)\n",
    "test_len = len(primary_data) - train_len\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(primary_data, [train_len, test_len])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "\n",
    "num_feats = np.array([train_set[i][0]for i in range(len(train_set))])\n",
    "num_labels = np.array([train_set[i][1]for i in range(len(train_set))])\n",
    "\n",
    "a = iter(train_loader)\n",
    "b = next(a)\n",
    "b = np.asarray(b)\n",
    "print(b.shape)\n",
    "inp_size = (b[0].shape)[1]\n",
    "print(inp_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4475200\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "ab = list(itertools.chain(*[i[0] for i in clean_truth_data]))\n",
    "print(len(ab))\n",
    "ab = set([int(i) for i in ab])\n",
    "emb_dim = len(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "RecurrentClassifier(\n",
      "  (embedding): Embedding(13784, 400)\n",
      "  (rnn): LSTM(400, 50, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (fc1): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RecurrentClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, input_size, hidden_size, output_size, num_layers, dropout=0.3):\n",
    "        super(RecurrentClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(embedding_dim, input_size)\n",
    "        self.rnn = nn.LSTM(input_size, \n",
    "                            hidden_size,\n",
    "                            num_layers,\n",
    "                            batch_first = True,\n",
    "                            dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, (hidden, cell) = self.rnn(x)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1, :, :]), dim=1))\n",
    "        x = self.fc1(hidden)\n",
    "        x = self.dropout(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "max_len = len(train_set[1][0])\n",
    "ref_check = 5\n",
    "print(max_len)\n",
    "\n",
    "net = RecurrentClassifier(emb_dim, int(inp_size), 50, ref_check, 2, dropout=0.2)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR = 1e-3\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR, weight_decay=5e-3)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 1000\n",
    "losses = []\n",
    "\n",
    "for step in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inp, labels = data\n",
    "        inp, labels = inp.long(), labels.long()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inp)\n",
    "        cost = loss_func(outputs, labels)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += cost.item()\n",
    "    print(f'Epoch: {step}   Training Loss: {running_loss/len(train_loader)}')\n",
    "print('Training Complete')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
