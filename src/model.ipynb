{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'True': 0, 'False': 1, 'Mostly False': 2, 'Mostly True': 3, 'Pants on Fire!': 4}\n",
      "(array([ 8502.,   787.,  7980., 10387.,  2915.,     0.,  4040.,  1729.,\n",
      "           0.,     0.,     0.,  3944.,  6442.,  7842.,  6275.,  7639.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.]), tensor(0., dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "from data import data_dict, DIRECTORY\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import nltk\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "class BiasDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file, root, x_col, y_col, meta_columns, label_idx = -1):\n",
    "        self.data = pd.read_csv(file)\n",
    "        self.og_data = self.data\n",
    "        self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        self.tokenized_data, self.base_ref = self.tokenizer(self.data, [x_col])\n",
    "        self.clean_data, self.errors = self.word_vector(self.tokenized_data.drop(meta_columns, axis=1), self.base_ref, [x_col])\n",
    "        \n",
    "        self.data = self.vectorize(self.clean_data, [y_col])\n",
    "        self.data = self.padding(self.padding(self.data, [x_col]), [x_col])\n",
    "        self.data = self.data.to_numpy()\n",
    "\n",
    "        max_len = max([len(i[0]) for i in self.data])\n",
    "\n",
    "        for idx, ent_item in enumerate(self.data):\n",
    "            add_array = np.array([0]*(max_len-len(ent_item[0])))\n",
    "            self.data[idx][0] = np.concatenate((ent_item[0], add_array))\n",
    "\n",
    "        \n",
    "        self.root = root\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def transform(self, data, col_names):\n",
    "        for col in col_names:\n",
    "            uniques = [list(set(data[col].values))][0]\n",
    "            uniques = [x for x in uniques if str(x) != 'nan']\n",
    "            one_hot = np.identity(len(uniques))\n",
    "            one_hot = [str(i) for i in one_hot.tolist()]\n",
    "            one_dict = dict(zip(uniques, one_hot))\n",
    "            data = data.replace(one_dict)\n",
    "        return data\n",
    "\n",
    "    def format_text(self, token):\n",
    "        clean_token = ''.join(chr for chr in token if chr.isalnum() and chr.isalpha())\n",
    "        return clean_token\n",
    "\n",
    "    def tokenizer(self, data, text_cols, window=2):\n",
    "        all_text = set()\n",
    "        transformed_data = data\n",
    "        for x in text_cols:\n",
    "            for idx, entry in enumerate(data[x]):\n",
    "                clean_entry = list(map(self.format_text, (word for word in entry)))\n",
    "                append_all_text = set()\n",
    "                for y, char in enumerate(clean_entry):\n",
    "                    if char == '':\n",
    "                        clean_entry[y] = ' '\n",
    "                all_words = ''.join(i.lower() for i in clean_entry)\n",
    "                transformed_data[x][idx] = all_words\n",
    "                \n",
    "                for m in set(all_words.split(' ')):\n",
    "                    all_text.add(m)\n",
    "\n",
    "        return transformed_data, dict(zip(list(all_text), [z for z in range(len(all_text))]))\n",
    "\n",
    "    def word_vector(self, data, ref, text_cols):\n",
    "        bag_dataset = data\n",
    "        errors = []\n",
    "        for row in text_cols:\n",
    "            for idx, entry in enumerate(data[row]):\n",
    "                list_entry = entry.split(' ')\n",
    "                try:\n",
    "                    vector = torch.FloatTensor([ref[word.lower()] for word in list_entry])\n",
    "                except:\n",
    "                    errors.append([list_entry, idx])\n",
    "                bag_dataset[row][idx] = vector\n",
    "        \n",
    "        return bag_dataset, errors\n",
    "\n",
    "    def vectorize(self, data_inp, columns):\n",
    "        data = data_inp\n",
    "        for column in columns:\n",
    "            labels = list(data[column].unique())\n",
    "            ref = dict(zip(data[column].unique(), [i for i in range(len(labels))]))\n",
    "            print(ref)\n",
    "            for idx, val in enumerate(data[column]):\n",
    "                vectorized = ref[data[column][idx]]\n",
    "                data[column][idx] = torch.tensor(vectorized, dtype=float)\n",
    "        return data\n",
    "    \n",
    "    def padding(self, data, x_column):\n",
    "        deep_copy = data\n",
    "        max_len = max([len(i) for i in deep_copy[x_column]])\n",
    "        for idx, i in enumerate(deep_copy[x_column]):\n",
    "            if len(i) != max_len:\n",
    "                flag = False\n",
    "                print(max_len-len(i))\n",
    "                print(deep_copy[x_column][idx], torch.cat((deep_copy[x_column][idx], torch.FloatTensor([0]*(max_len-len(i))))))\n",
    "                deep_copy[x_column][idx] = torch.cat((deep_copy[x_column][idx], torch.FloatTensor([0]*(max_len-len(i)))))\n",
    "            else:\n",
    "                flag = True\n",
    "                pass\n",
    "        else:\n",
    "            return deep_copy\n",
    "\n",
    "    \n",
    "    def __len__ (self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__ (self, idx):\n",
    "        self.transpose_data = self.data\n",
    "        self.transpose_data = self.transpose_data.transpose()\n",
    "        x_data = self.transpose_data[0]\n",
    "        y_data = self.transpose_data[1]\n",
    "\n",
    "        return x_data[idx], y_data[idx]\n",
    "    \n",
    "\n",
    "truth_data = BiasDataset(data_dict['politifact_clean'], DIRECTORY, 'statement', 'veracity', ['source', 'link'])\n",
    "print(truth_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The national economic recovery has led to higher than expected tax revenues and projected budget surpluses in nearly every state in the nation, including Wisconsin.\"\n",
      "166\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    1  123  323\n",
      " 1140   13  581    4  277   16 1713   28 1141    6 1349   85 5498    2\n",
      "  166   71   21    2    1  177  402   92]\n",
      "{'True': 0, 'Mostly False': 1, 'False': 2, 'Mostly True': 3, 'Pants on Fire!': 4}\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class PreprocessingDataset(Dataset):\n",
    "    def __init__(self, file, root, x_col, y_col, meta_columns, label_idx = -1):\n",
    "        self.x_col = x_col\n",
    "        self.y_col = y_col\n",
    "        self.data = pd.read_csv(file)\n",
    "        self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "        self.data = self.data.drop(meta_columns, axis=1)\n",
    "\n",
    "        # self.data, self.base_ref = self.tokenizer(self.data, [x_col])\n",
    "        self.x_data = self.data[x_col]\n",
    "        self.max_len = max([len(i) for i in self.x_data])\n",
    "\n",
    "        self.x_data = self.word_vector(self.x_data)\n",
    "        self.data[x_col] = [torch.FloatTensor(i) for i in self.x_data]\n",
    "        self.data = self.vectorize(self.data, [y_col])\n",
    "\n",
    "        self.data = self.data.to_numpy()\n",
    "\n",
    "        self.root = root\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def format_text(self, token):\n",
    "        clean_token = ''.join(chr for chr in token if chr.isalnum() and chr.isalpha())\n",
    "        return clean_token\n",
    "\n",
    "    def word_vector(self, data):\n",
    "        x_data = data\n",
    "        x_data = list(x_data)\n",
    "        maximum_length = 0\n",
    "        max_idx = 0\n",
    "        for idx, i in enumerate(x_data):\n",
    "\n",
    "            if len(i) > maximum_length:\n",
    "                maximum_length = len(i)\n",
    "                max_idx = idx\n",
    "        \n",
    "        t = Tokenizer()\n",
    "        t.fit_on_texts(x_data)\n",
    "        sequences = t.texts_to_sequences(x_data)\n",
    "        sequences = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=maximum_length)\n",
    "        print(x_data[0])\n",
    "        print(len(x_data[0]))\n",
    "        print(sequences[0])\n",
    "\n",
    "        return sequences\n",
    "\n",
    "\n",
    "    def vectorize(self, data_inp, columns):\n",
    "        data = data_inp\n",
    "        for column in columns:\n",
    "            labels = list(data[column].unique())\n",
    "            ref = dict(zip(data[column].unique(), [i for i in range(len(labels))]))\n",
    "            print(ref)\n",
    "            for idx, val in enumerate(data[column]):\n",
    "                vectorized = ref[data[column][idx]]\n",
    "                data[column][idx] = torch.tensor(vectorized, dtype=float)\n",
    "        return data\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__ (self, idx):\n",
    "        \n",
    "        self.transpose_data = self.data\n",
    "        self.transpose_data = self.transpose_data.transpose()\n",
    "        x_data = self.transpose_data[0]\n",
    "        y_data = self.transpose_data[1]\n",
    "\n",
    "        return x_data[idx], y_data[idx]\n",
    "\n",
    "clean_truth_data = PreprocessingDataset(data_dict['politifact_clean'], DIRECTORY, 'statement', 'veracity', ['source', 'link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 1.2300e+02, 3.2300e+02,\n",
      "        1.1400e+03, 1.3000e+01, 5.8100e+02, 4.0000e+00, 2.7700e+02, 1.6000e+01,\n",
      "        1.7130e+03, 2.8000e+01, 1.1410e+03, 6.0000e+00, 1.3490e+03, 8.5000e+01,\n",
      "        5.4980e+03, 2.0000e+00, 1.6600e+02, 7.1000e+01, 2.1000e+01, 2.0000e+00,\n",
      "        1.0000e+00, 1.7700e+02, 4.0200e+02, 9.2000e+01]), tensor(0., dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "print(clean_truth_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-4fe4853537fe>:18: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  num_feats = np.array([train_set[i][0]for i in range(len(train_set))])\n",
      "<ipython-input-19-4fe4853537fe>:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  num_feats = np.array([train_set[i][0]for i in range(len(train_set))])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-4fe4853537fe>:27: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  b = np.array(next(a))\n",
      "<ipython-input-19-4fe4853537fe>:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  b = np.array(next(a))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "primary_data = clean_truth_data #secondary option of truth_data\n",
    "\n",
    "train_len = int(len(primary_data)*0.8)\n",
    "test_len = len(primary_data) - train_len\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(primary_data, [train_len, test_len])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# print(len(train_set))\n",
    "# print(len(test_set))\n",
    "# print(len(primary_data))\n",
    "\n",
    "num_feats = np.array([train_set[i][0]for i in range(len(train_set))])\n",
    "num_labels = np.array([train_set[i][1]for i in range(len(train_set))])\n",
    "\n",
    "\n",
    "# print(num_feats.shape)\n",
    "# print(num_labels.shape)\n",
    "\n",
    "if primary_data == clean_truth_data:\n",
    "    a = iter(train_loader)\n",
    "    b = np.array(next(a))\n",
    "    inp_size = (b[0].shape)[1]\n",
    "else:\n",
    "    inp_size = str(num_feats[0].shape)[-5:-2]\n",
    "\n",
    "print(inp_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "FeedForward(\n",
      "  (fc1): Linear(in_features=400, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=75, bias=True)\n",
      "  (fc4): Linear(in_features=75, out_features=20, bias=True)\n",
      "  (fc6): Linear(in_features=20, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, kernel_size=4):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 100)\n",
    "        self.fc3 = nn.Linear(100, 75)        \n",
    "        self.fc4 = nn.Linear(75, 20)\n",
    "        self.fc6 = nn.Linear(20, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class RecurrentClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout=0.3):\n",
    "        super(RecurrentClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, \n",
    "                            hidden_size,\n",
    "                            num_layers,\n",
    "                            dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, (hidden, cell) = self.rnn(x)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1, :, :]), dim=1))\n",
    "        x = self.fc1(hidden)\n",
    "        x = self.dropout(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "max_len = len(train_set[1][0])\n",
    "ref_check = 5\n",
    "print(max_len)\n",
    "\n",
    "# net = RecurrentClassifier(int(inp_size), 256, ref_check, 3, dropout=0.2)\n",
    "net = FeedForward(ref_check, inp_size)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0   Training Loss: 1.6022040946143015\n",
      "Epoch: 1   Training Loss: 1.5965734141213552\n",
      "Epoch: 2   Training Loss: 1.594014415570668\n",
      "Epoch: 3   Training Loss: 1.59106639112745\n",
      "Epoch: 4   Training Loss: 1.5878231644630432\n",
      "Epoch: 5   Training Loss: 1.584990439244679\n",
      "Epoch: 6   Training Loss: 1.5818960785865783\n",
      "Epoch: 7   Training Loss: 1.5791167744568415\n",
      "Epoch: 8   Training Loss: 1.5764760971069336\n",
      "Epoch: 9   Training Loss: 1.5728277700287956\n",
      "Epoch: 10   Training Loss: 1.5707032663481577\n",
      "Epoch: 11   Training Loss: 1.5673700290066854\n",
      "Epoch: 12   Training Loss: 1.5629774570465087\n",
      "Epoch: 13   Training Loss: 1.5592061919825417\n",
      "Epoch: 14   Training Loss: 1.5561555334499904\n",
      "Epoch: 15   Training Loss: 1.5495520685400281\n",
      "Epoch: 16   Training Loss: 1.545477521419525\n",
      "Epoch: 17   Training Loss: 1.5395560349736894\n",
      "Epoch: 18   Training Loss: 1.5344181162970407\n",
      "Epoch: 19   Training Loss: 1.531307884625026\n",
      "Epoch: 20   Training Loss: 1.522511110986982\n",
      "Epoch: 21   Training Loss: 1.519940481015614\n",
      "Epoch: 22   Training Loss: 1.5122249722480774\n",
      "Epoch: 23   Training Loss: 1.500342562368938\n",
      "Epoch: 24   Training Loss: 1.4932164021900722\n",
      "Epoch: 25   Training Loss: 1.484106662443706\n",
      "Epoch: 26   Training Loss: 1.479310783318111\n",
      "Epoch: 27   Training Loss: 1.4705659057412828\n",
      "Epoch: 28   Training Loss: 1.46306779725211\n",
      "Epoch: 29   Training Loss: 1.4573510135923113\n",
      "Epoch: 30   Training Loss: 1.4537512685571399\n",
      "Epoch: 31   Training Loss: 1.4429874224322183\n",
      "Epoch: 32   Training Loss: 1.433850028685161\n",
      "Epoch: 33   Training Loss: 1.4302015653678348\n",
      "Epoch: 34   Training Loss: 1.4233364828995296\n",
      "Epoch: 35   Training Loss: 1.4119340419769286\n",
      "Epoch: 36   Training Loss: 1.4070780737059456\n",
      "Epoch: 37   Training Loss: 1.3999575299876077\n",
      "Epoch: 38   Training Loss: 1.3886114180088043\n",
      "Epoch: 39   Training Loss: 1.3730908036231995\n",
      "Epoch: 40   Training Loss: 1.3696561574935913\n",
      "Epoch: 41   Training Loss: 1.360425706420626\n",
      "Epoch: 42   Training Loss: 1.3593338455472674\n",
      "Epoch: 43   Training Loss: 1.3509101867675781\n",
      "Epoch: 44   Training Loss: 1.3449088862964085\n",
      "Epoch: 45   Training Loss: 1.352711717571531\n",
      "Epoch: 46   Training Loss: 1.3408555209636688\n",
      "Epoch: 47   Training Loss: 1.3169852656977517\n",
      "Epoch: 48   Training Loss: 1.3148529384817396\n",
      "Epoch: 49   Training Loss: 1.3012666727815356\n",
      "Epoch: 50   Training Loss: 1.3095867369856153\n",
      "Epoch: 51   Training Loss: 1.2931030469281333\n",
      "Epoch: 52   Training Loss: 1.2805396624973842\n",
      "Epoch: 53   Training Loss: 1.2777012484414236\n",
      "Epoch: 54   Training Loss: 1.2816624811717443\n",
      "Epoch: 55   Training Loss: 1.2844878673553466\n",
      "Epoch: 56   Training Loss: 1.2503874012402125\n",
      "Epoch: 57   Training Loss: 1.2615018137863705\n",
      "Epoch: 58   Training Loss: 1.2496245997292654\n",
      "Epoch: 59   Training Loss: 1.250189219202314\n",
      "Epoch: 60   Training Loss: 1.2533658606665474\n",
      "Epoch: 61   Training Loss: 1.2319412980760847\n",
      "Epoch: 62   Training Loss: 1.2236986151763372\n",
      "Epoch: 63   Training Loss: 1.2187880430902753\n",
      "Epoch: 64   Training Loss: 1.225433794089726\n",
      "Epoch: 65   Training Loss: 1.2168058029242925\n",
      "Epoch: 66   Training Loss: 1.2061260917357035\n",
      "Epoch: 67   Training Loss: 1.2083932123013905\n",
      "Epoch: 68   Training Loss: 1.2149862676858902\n",
      "Epoch: 69   Training Loss: 1.2003031981842858\n",
      "Epoch: 70   Training Loss: 1.1965148380824497\n",
      "Epoch: 71   Training Loss: 1.1801451976810182\n",
      "Epoch: 72   Training Loss: 1.1765715339354106\n",
      "Epoch: 73   Training Loss: 1.1886586219072341\n",
      "Epoch: 74   Training Loss: 1.1685026309319906\n",
      "Epoch: 75   Training Loss: 1.152979987859726\n",
      "Epoch: 76   Training Loss: 1.1444070573363985\n",
      "Epoch: 77   Training Loss: 1.1590813968862805\n",
      "Epoch: 78   Training Loss: 1.1717028779642922\n",
      "Epoch: 79   Training Loss: 1.169435316324234\n",
      "Epoch: 80   Training Loss: 1.1581523873976298\n",
      "Epoch: 81   Training Loss: 1.1342877362455641\n",
      "Epoch: 82   Training Loss: 1.1377702674695425\n",
      "Epoch: 83   Training Loss: 1.1539648251874106\n",
      "Epoch: 84   Training Loss: 1.1406753484691892\n",
      "Epoch: 85   Training Loss: 1.1240587119545256\n",
      "Epoch: 86   Training Loss: 1.1469028579337257\n",
      "Epoch: 87   Training Loss: 1.1534115067550115\n",
      "Epoch: 88   Training Loss: 1.1400011654411044\n",
      "Epoch: 89   Training Loss: 1.1397061829056059\n",
      "Epoch: 90   Training Loss: 1.1142657871757236\n",
      "Epoch: 91   Training Loss: 1.1121751470225198\n",
      "Epoch: 92   Training Loss: 1.1053228910480226\n",
      "Epoch: 93   Training Loss: 1.0832541831902096\n",
      "Epoch: 94   Training Loss: 1.0844007130180087\n",
      "Epoch: 95   Training Loss: 1.090247654063361\n",
      "Epoch: 96   Training Loss: 1.1065408366067069\n",
      "Epoch: 97   Training Loss: 1.0893750578165053\n",
      "Epoch: 98   Training Loss: 1.0907498717308044\n",
      "Epoch: 99   Training Loss: 1.095594049351556\n",
      "Epoch: 100   Training Loss: 1.0918450989893504\n",
      "Epoch: 101   Training Loss: 1.0963715876851763\n",
      "Epoch: 102   Training Loss: 1.0845318104539599\n",
      "Epoch: 103   Training Loss: 1.0751495731728418\n",
      "Epoch: 104   Training Loss: 1.0842369377613068\n",
      "Epoch: 105   Training Loss: 1.0755322005067554\n",
      "Epoch: 106   Training Loss: 1.089381395493235\n",
      "Epoch: 107   Training Loss: 1.0850177862814494\n",
      "Epoch: 108   Training Loss: 1.0535224620785033\n",
      "Epoch: 109   Training Loss: 1.0404368958302908\n",
      "Epoch: 110   Training Loss: 1.0437724053859712\n",
      "Epoch: 111   Training Loss: 1.044682914018631\n",
      "Epoch: 112   Training Loss: 1.0631895835910525\n",
      "Epoch: 113   Training Loss: 1.057584182705198\n",
      "Epoch: 114   Training Loss: 1.053190398641995\n",
      "Epoch: 115   Training Loss: 1.0754746364695684\n",
      "Epoch: 116   Training Loss: 1.095549854210445\n",
      "Epoch: 117   Training Loss: 1.06727203641619\n",
      "Epoch: 118   Training Loss: 1.0382747845990317\n",
      "Epoch: 119   Training Loss: 1.0413507372140884\n",
      "Epoch: 120   Training Loss: 1.0427880551133837\n",
      "Epoch: 121   Training Loss: 1.0106070003339223\n",
      "Epoch: 122   Training Loss: 1.017371643441064\n",
      "Epoch: 123   Training Loss: 1.0461950387273515\n",
      "Epoch: 124   Training Loss: 1.0331352123192379\n",
      "Epoch: 125   Training Loss: 1.0325030854770116\n",
      "Epoch: 126   Training Loss: 1.074327369247164\n",
      "Epoch: 127   Training Loss: 1.0640264894281115\n",
      "Epoch: 128   Training Loss: 1.0584758430719376\n",
      "Epoch: 129   Training Loss: 1.0205218481166023\n",
      "Epoch: 130   Training Loss: 1.0023458353110721\n",
      "Epoch: 131   Training Loss: 0.9935966168131147\n",
      "Epoch: 132   Training Loss: 1.0026493519544601\n",
      "Epoch: 133   Training Loss: 1.0048209322350365\n",
      "Epoch: 134   Training Loss: 1.016942394205502\n",
      "Epoch: 135   Training Loss: 1.0329684453351158\n",
      "Epoch: 136   Training Loss: 1.0293092876672745\n",
      "Epoch: 137   Training Loss: 1.0198087871074677\n",
      "Epoch: 138   Training Loss: 1.0087942685399736\n",
      "Epoch: 139   Training Loss: 1.0134021346058164\n",
      "Epoch: 140   Training Loss: 0.9917267646108355\n",
      "Epoch: 141   Training Loss: 0.994182791028704\n",
      "Epoch: 142   Training Loss: 0.9732421049049922\n",
      "Epoch: 143   Training Loss: 0.9747414043971471\n",
      "Epoch: 144   Training Loss: 0.978103592140334\n",
      "Epoch: 145   Training Loss: 1.028813773393631\n",
      "Epoch: 146   Training Loss: 1.032041892835072\n",
      "Epoch: 147   Training Loss: 1.0661895594426565\n",
      "Epoch: 148   Training Loss: 0.9972638547420501\n",
      "Epoch: 149   Training Loss: 0.9814624816179276\n",
      "Epoch: 150   Training Loss: 0.9896385490894317\n",
      "Epoch: 151   Training Loss: 0.9739893249103001\n",
      "Epoch: 152   Training Loss: 0.9849518469401768\n",
      "Epoch: 153   Training Loss: 0.992923931138856\n",
      "Epoch: 154   Training Loss: 1.0005543253251485\n",
      "Epoch: 155   Training Loss: 0.9986309238842556\n",
      "Epoch: 156   Training Loss: 0.9707077924694334\n",
      "Epoch: 157   Training Loss: 1.0040024037872042\n",
      "Epoch: 158   Training Loss: 1.0083583052669252\n",
      "Epoch: 159   Training Loss: 0.9853173307010106\n",
      "Epoch: 160   Training Loss: 0.9754360646009446\n",
      "Epoch: 161   Training Loss: 0.9960871411221368\n",
      "Epoch: 162   Training Loss: 0.997203369651522\n",
      "Epoch: 163   Training Loss: 0.9552036289657865\n",
      "Epoch: 164   Training Loss: 0.9540538025753839\n",
      "Epoch: 165   Training Loss: 0.991162667955671\n",
      "Epoch: 166   Training Loss: 0.9805175691843033\n",
      "Epoch: 167   Training Loss: 0.9745983472892217\n",
      "Epoch: 168   Training Loss: 0.9381049909761974\n",
      "Epoch: 169   Training Loss: 0.9555287233420781\n",
      "Epoch: 170   Training Loss: 0.9527408199650901\n",
      "Epoch: 171   Training Loss: 1.0053713811295373\n",
      "Epoch: 172   Training Loss: 1.0041712978056498\n",
      "Epoch: 173   Training Loss: 0.9731105953454972\n",
      "Epoch: 174   Training Loss: 0.9382387237889426\n",
      "Epoch: 175   Training Loss: 0.9283270146165575\n",
      "Epoch: 176   Training Loss: 0.9274140949760165\n",
      "Epoch: 177   Training Loss: 0.9491123110055923\n",
      "Epoch: 178   Training Loss: 0.9674704070602145\n",
      "Epoch: 179   Training Loss: 1.0001011665378299\n",
      "Epoch: 180   Training Loss: 1.0084391853639059\n",
      "Epoch: 181   Training Loss: 0.9969041126114981\n",
      "Epoch: 182   Training Loss: 0.9716343228306089\n",
      "Epoch: 183   Training Loss: 0.9537936734301703\n",
      "Epoch: 184   Training Loss: 0.9511604006801333\n",
      "Epoch: 185   Training Loss: 0.9532639741897583\n",
      "Epoch: 186   Training Loss: 0.9551684264625822\n",
      "Epoch: 187   Training Loss: 0.9816098600625992\n",
      "Epoch: 188   Training Loss: 0.9460780680179596\n",
      "Epoch: 189   Training Loss: 0.924363740852901\n",
      "Epoch: 190   Training Loss: 0.9432805929865156\n",
      "Epoch: 191   Training Loss: 0.9368143694741385\n",
      "Epoch: 192   Training Loss: 0.939050104362624\n",
      "Epoch: 193   Training Loss: 0.9688317673546928\n",
      "Epoch: 194   Training Loss: 1.0007729704890933\n",
      "Epoch: 195   Training Loss: 0.9396637533392225\n",
      "Epoch: 196   Training Loss: 0.9178017177752086\n",
      "Epoch: 197   Training Loss: 0.9062452971935272\n",
      "Epoch: 198   Training Loss: 0.9276983001402446\n",
      "Epoch: 199   Training Loss: 0.9481882350785392\n",
      "Epoch: 200   Training Loss: 0.9175957888364792\n",
      "Epoch: 201   Training Loss: 0.9543399270091738\n",
      "Epoch: 202   Training Loss: 0.9676952004432678\n",
      "Epoch: 203   Training Loss: 0.9560828302587782\n",
      "Epoch: 204   Training Loss: 0.9974479751927512\n",
      "Epoch: 205   Training Loss: 0.9658550109182086\n",
      "Epoch: 206   Training Loss: 0.9117572665214538\n",
      "Epoch: 207   Training Loss: 0.9015096766608102\n",
      "Epoch: 208   Training Loss: 0.9034568173544747\n",
      "Epoch: 209   Training Loss: 0.9053817706448691\n",
      "Epoch: 210   Training Loss: 0.9122944482735225\n",
      "Epoch: 211   Training Loss: 0.9271821656397411\n",
      "Epoch: 212   Training Loss: 0.9429623420749392\n",
      "Epoch: 213   Training Loss: 0.915693365250315\n",
      "Epoch: 214   Training Loss: 0.9924044592039926\n",
      "Epoch: 215   Training Loss: 0.9924849263259343\n",
      "Epoch: 216   Training Loss: 0.9368646442890167\n",
      "Epoch: 217   Training Loss: 0.887159384574209\n",
      "Epoch: 218   Training Loss: 0.8897404406751905\n",
      "Epoch: 219   Training Loss: 0.9014273886169706\n",
      "Epoch: 220   Training Loss: 0.9108110525778361\n",
      "Epoch: 221   Training Loss: 0.9340258585555213\n",
      "Epoch: 222   Training Loss: 0.9581956948552813\n",
      "Epoch: 223   Training Loss: 0.94813500557627\n",
      "Epoch: 224   Training Loss: 0.9621150412729809\n",
      "Epoch: 225   Training Loss: 0.9609433220965522\n",
      "Epoch: 226   Training Loss: 0.9090332580464227\n",
      "Epoch: 227   Training Loss: 0.8833965837955475\n",
      "Epoch: 228   Training Loss: 0.9146209529467991\n",
      "Epoch: 229   Training Loss: 0.9046916804143361\n",
      "Epoch: 230   Training Loss: 0.8913506746292115\n",
      "Epoch: 231   Training Loss: 0.9256787240505219\n",
      "Epoch: 232   Training Loss: 0.9567536669118064\n",
      "Epoch: 233   Training Loss: 0.9470765309674399\n",
      "Epoch: 234   Training Loss: 0.925765500324113\n",
      "Epoch: 235   Training Loss: 0.9485914490052632\n",
      "Epoch: 236   Training Loss: 0.913634004337447\n",
      "Epoch: 237   Training Loss: 0.9138339225734983\n",
      "Epoch: 238   Training Loss: 0.9019767212016242\n",
      "Epoch: 239   Training Loss: 0.9247122773102352\n",
      "Epoch: 240   Training Loss: 0.9223010080201285\n",
      "Epoch: 241   Training Loss: 0.9310018679925374\n",
      "Epoch: 242   Training Loss: 0.9146710842847824\n",
      "Epoch: 243   Training Loss: 0.9070204194102969\n",
      "Epoch: 244   Training Loss: 0.8927400035517556\n",
      "Epoch: 245   Training Loss: 0.8737480325358254\n",
      "Epoch: 246   Training Loss: 0.8703360800232206\n",
      "Epoch: 247   Training Loss: 0.8511426478624344\n",
      "Epoch: 248   Training Loss: 0.8890120442424502\n",
      "Epoch: 249   Training Loss: 0.9061574680464608\n",
      "Epoch: 250   Training Loss: 0.9026699994291578\n",
      "Epoch: 251   Training Loss: 0.9325305231979915\n",
      "Epoch: 252   Training Loss: 0.9849515544516699\n",
      "Epoch: 253   Training Loss: 0.9333295507090432\n",
      "Epoch: 254   Training Loss: 0.8994193528379713\n",
      "Epoch: 255   Training Loss: 0.8935352955545698\n",
      "Epoch: 256   Training Loss: 0.9145291230508259\n",
      "Epoch: 257   Training Loss: 0.9254814735480718\n",
      "Epoch: 258   Training Loss: 0.9020461520978382\n",
      "Epoch: 259   Training Loss: 0.8876205887113299\n",
      "Epoch: 260   Training Loss: 0.8791896688086646\n",
      "Epoch: 261   Training Loss: 0.8912467743669238\n",
      "Epoch: 262   Training Loss: 0.9421094102518899\n",
      "Epoch: 263   Training Loss: 0.8935606211423874\n",
      "Epoch: 264   Training Loss: 0.9097171272550311\n",
      "Epoch: 265   Training Loss: 0.894650142959186\n",
      "Epoch: 266   Training Loss: 0.8635896384716034\n",
      "Epoch: 267   Training Loss: 0.8745306159768785\n",
      "Epoch: 268   Training Loss: 0.8908760845661163\n",
      "Epoch: 269   Training Loss: 0.9101344734430313\n",
      "Epoch: 270   Training Loss: 0.8845187757696424\n",
      "Epoch: 271   Training Loss: 0.870440622312682\n",
      "Epoch: 272   Training Loss: 0.8654954829386302\n",
      "Epoch: 273   Training Loss: 0.9132679181439536\n",
      "Epoch: 274   Training Loss: 0.9115560382604599\n",
      "Epoch: 275   Training Loss: 0.9425920367240905\n",
      "Epoch: 276   Training Loss: 0.8994405520813805\n",
      "Epoch: 277   Training Loss: 0.9257420880453927\n",
      "Epoch: 278   Training Loss: 0.9033696515219553\n",
      "Epoch: 279   Training Loss: 0.8807835842881884\n",
      "Epoch: 280   Training Loss: 0.8565940128905433\n",
      "Epoch: 281   Training Loss: 0.8493755791868483\n",
      "Epoch: 282   Training Loss: 0.8596770401511874\n",
      "Epoch: 283   Training Loss: 0.8718784924064363\n",
      "Epoch: 284   Training Loss: 0.8578230632202966\n",
      "Epoch: 285   Training Loss: 0.8991635446037565\n",
      "Epoch: 286   Training Loss: 0.9079833856650761\n",
      "Epoch: 287   Training Loss: 0.8910826712846756\n",
      "Epoch: 288   Training Loss: 0.9211406431027821\n",
      "Epoch: 289   Training Loss: 0.8947816640138626\n",
      "Epoch: 290   Training Loss: 0.9211354093892233\n",
      "Epoch: 291   Training Loss: 0.8996337230716432\n",
      "Epoch: 292   Training Loss: 0.8570235946348735\n",
      "Epoch: 293   Training Loss: 0.8652473977633885\n",
      "Epoch: 294   Training Loss: 0.8630629671471459\n",
      "Epoch: 295   Training Loss: 0.9043008821351187\n",
      "Epoch: 296   Training Loss: 0.9098763478653772\n",
      "Epoch: 297   Training Loss: 0.8788945708956037\n",
      "Epoch: 298   Training Loss: 0.881095735515867\n",
      "Epoch: 299   Training Loss: 0.8974802787814822\n",
      "Epoch: 300   Training Loss: 0.9108936062880925\n",
      "Epoch: 301   Training Loss: 0.9071693467242378\n",
      "Epoch: 302   Training Loss: 0.864707847578185\n",
      "Epoch: 303   Training Loss: 0.8560515684740884\n",
      "Epoch: 304   Training Loss: 0.8288687608071736\n",
      "Epoch: 305   Training Loss: 0.8635699157203947\n",
      "Epoch: 306   Training Loss: 0.891844448872975\n",
      "Epoch: 307   Training Loss: 0.8790516563824244\n",
      "Epoch: 308   Training Loss: 0.8847318325723921\n",
      "Epoch: 309   Training Loss: 0.8827839940786362\n",
      "Epoch: 310   Training Loss: 0.8486263513565063\n",
      "Epoch: 311   Training Loss: 0.8556106239557266\n",
      "Epoch: 312   Training Loss: 0.9158358808074679\n",
      "Epoch: 313   Training Loss: 0.9036136286599296\n",
      "Epoch: 314   Training Loss: 0.863737341761589\n",
      "Epoch: 315   Training Loss: 0.8281266650983266\n",
      "Epoch: 316   Training Loss: 0.8133643827268056\n",
      "Epoch: 317   Training Loss: 0.8675438182694571\n",
      "Epoch: 318   Training Loss: 0.9617836994784219\n",
      "Epoch: 319   Training Loss: 0.9140655044998441\n",
      "Epoch: 320   Training Loss: 0.8926497233765466\n",
      "Epoch: 321   Training Loss: 0.8646903629813876\n",
      "Epoch: 322   Training Loss: 0.8729395883423942\n",
      "Epoch: 323   Training Loss: 0.8510209296430861\n",
      "Epoch: 324   Training Loss: 0.8785212044204984\n",
      "Epoch: 325   Training Loss: 0.8587308692080634\n",
      "Epoch: 326   Training Loss: 0.8862207893814359\n",
      "Epoch: 327   Training Loss: 0.8874034242970603\n",
      "Epoch: 328   Training Loss: 0.8711439635072435\n",
      "Epoch: 329   Training Loss: 0.8942202270030976\n",
      "Epoch: 330   Training Loss: 0.846612253359386\n",
      "Epoch: 331   Training Loss: 0.8362700441053935\n",
      "Epoch: 332   Training Loss: 0.8654193290642329\n",
      "Epoch: 333   Training Loss: 0.8606177066053663\n",
      "Epoch: 334   Training Loss: 0.8939042691673551\n",
      "Epoch: 335   Training Loss: 0.8713316955736705\n",
      "Epoch: 336   Training Loss: 0.8530889132193157\n",
      "Epoch: 337   Training Loss: 0.8731173651559012\n",
      "Epoch: 338   Training Loss: 0.8881647233452116\n",
      "Epoch: 339   Training Loss: 0.8554579424006599\n",
      "Epoch: 340   Training Loss: 0.8625338814088277\n",
      "Epoch: 341   Training Loss: 0.8768564207213265\n",
      "Epoch: 342   Training Loss: 0.8680938167231423\n",
      "Epoch: 343   Training Loss: 0.8279188143355506\n",
      "Epoch: 344   Training Loss: 0.8387168807642801\n",
      "Epoch: 345   Training Loss: 0.8797908949000495\n",
      "Epoch: 346   Training Loss: 0.8953095431838717\n",
      "Epoch: 347   Training Loss: 0.9195596652371543\n",
      "Epoch: 348   Training Loss: 0.8711202131850379\n",
      "Epoch: 349   Training Loss: 0.8481819250753948\n",
      "Epoch: 350   Training Loss: 0.8289799511432647\n",
      "Epoch: 351   Training Loss: 0.8472155973315239\n",
      "Epoch: 352   Training Loss: 0.8560061454772949\n",
      "Epoch: 353   Training Loss: 0.8533843619482858\n",
      "Epoch: 354   Training Loss: 0.8409148058720998\n",
      "Epoch: 355   Training Loss: 0.8583194434642791\n",
      "Epoch: 356   Training Loss: 0.8501747970070158\n",
      "Epoch: 357   Training Loss: 0.8118678007807051\n",
      "Epoch: 358   Training Loss: 0.8264765820332935\n",
      "Epoch: 359   Training Loss: 0.8662359629358564\n",
      "Epoch: 360   Training Loss: 0.9507621590580259\n",
      "Epoch: 361   Training Loss: 0.9521995948893683\n",
      "Epoch: 362   Training Loss: 0.9198386426482882\n",
      "Epoch: 363   Training Loss: 0.8570328716720853\n",
      "Epoch: 364   Training Loss: 0.8403891295194625\n",
      "Epoch: 365   Training Loss: 0.8282621009009224\n",
      "Epoch: 366   Training Loss: 0.8399689704179764\n",
      "Epoch: 367   Training Loss: 0.8499782068388803\n",
      "Epoch: 368   Training Loss: 0.847304887005261\n",
      "Epoch: 369   Training Loss: 0.8315413108893803\n",
      "Epoch: 370   Training Loss: 0.8489427230187825\n",
      "Epoch: 371   Training Loss: 0.9158212172133582\n",
      "Epoch: 372   Training Loss: 0.9206063445125308\n",
      "Epoch: 373   Training Loss: 0.8559883600899152\n",
      "Epoch: 374   Training Loss: 0.7972328762922968\n",
      "Epoch: 375   Training Loss: 0.8249699030603681\n",
      "Epoch: 376   Training Loss: 0.8555014810391834\n",
      "Epoch: 377   Training Loss: 0.8689871822084699\n",
      "Epoch: 378   Training Loss: 0.8719891399145127\n",
      "Epoch: 379   Training Loss: 0.8409525815929685\n",
      "Epoch: 380   Training Loss: 0.8414633951016834\n",
      "Epoch: 381   Training Loss: 0.8713752776384354\n",
      "Epoch: 382   Training Loss: 0.8664136001041958\n",
      "Epoch: 383   Training Loss: 0.8335680365562439\n",
      "Epoch: 384   Training Loss: 0.8591784511293684\n",
      "Epoch: 385   Training Loss: 0.8266853630542755\n",
      "Epoch: 386   Training Loss: 0.8462151395423072\n",
      "Epoch: 387   Training Loss: 0.8316835543939045\n",
      "Epoch: 388   Training Loss: 0.8450376923595156\n",
      "Epoch: 389   Training Loss: 0.8783483292375293\n",
      "Epoch: 390   Training Loss: 0.8387097541775023\n",
      "Epoch: 391   Training Loss: 0.840296299968447\n",
      "Epoch: 392   Training Loss: 0.8563022217580251\n",
      "Epoch: 393   Training Loss: 0.8519948682614735\n",
      "Epoch: 394   Training Loss: 0.8636181167193822\n",
      "Epoch: 395   Training Loss: 0.8400755281959261\n",
      "Epoch: 396   Training Loss: 0.8525399127176829\n",
      "Epoch: 397   Training Loss: 0.8501627377101353\n",
      "Epoch: 398   Training Loss: 0.8373407248939787\n",
      "Epoch: 399   Training Loss: 0.8105798333883285\n",
      "Epoch: 400   Training Loss: 0.8213463983365468\n",
      "Epoch: 401   Training Loss: 0.8206688267844063\n",
      "Epoch: 402   Training Loss: 0.8466014815228325\n",
      "Epoch: 403   Training Loss: 0.8624775426728385\n",
      "Epoch: 404   Training Loss: 0.8721224337816238\n",
      "Epoch: 405   Training Loss: 0.8686964056321553\n",
      "Epoch: 406   Training Loss: 0.8731272965669632\n",
      "Epoch: 407   Training Loss: 0.8822894649846214\n",
      "Epoch: 408   Training Loss: 0.8364039765936988\n",
      "Epoch: 409   Training Loss: 0.8171585129840033\n",
      "Epoch: 410   Training Loss: 0.7901192903518677\n",
      "Epoch: 411   Training Loss: 0.7880613952875137\n",
      "Epoch: 412   Training Loss: 0.8768696257046291\n",
      "Epoch: 413   Training Loss: 0.8686925964696067\n",
      "Epoch: 414   Training Loss: 0.8947473130055836\n",
      "Epoch: 415   Training Loss: 0.887099460193089\n",
      "Epoch: 416   Training Loss: 0.846953592981611\n",
      "Epoch: 417   Training Loss: 0.8411347508430481\n",
      "Epoch: 418   Training Loss: 0.8355014379535403\n",
      "Epoch: 419   Training Loss: 0.8344607787472861\n",
      "Epoch: 420   Training Loss: 0.8223568303244454\n",
      "Epoch: 421   Training Loss: 0.839023478116308\n",
      "Epoch: 422   Training Loss: 0.89712535909244\n",
      "Epoch: 423   Training Loss: 0.8640397059065955\n",
      "Epoch: 424   Training Loss: 0.8416127002664975\n",
      "Epoch: 425   Training Loss: 0.8204689387764249\n",
      "Epoch: 426   Training Loss: 0.8397314067397799\n",
      "Epoch: 427   Training Loss: 0.8500250786542892\n",
      "Epoch: 428   Training Loss: 0.8479582722697939\n",
      "Epoch: 429   Training Loss: 0.7994859495333263\n",
      "Epoch: 430   Training Loss: 0.7918186549629483\n",
      "Epoch: 431   Training Loss: 0.801234160576548\n",
      "Epoch: 432   Training Loss: 0.8003286165850503\n",
      "Epoch: 433   Training Loss: 0.8428104898759297\n",
      "Epoch: 434   Training Loss: 0.8664968401193619\n",
      "Epoch: 435   Training Loss: 0.8770869668040957\n",
      "Epoch: 436   Training Loss: 0.8766125742878232\n",
      "Epoch: 437   Training Loss: 0.8852225261075156\n",
      "Epoch: 438   Training Loss: 0.8505967553172793\n",
      "Epoch: 439   Training Loss: 0.8263210722378322\n",
      "Epoch: 440   Training Loss: 0.8125441010509219\n",
      "Epoch: 441   Training Loss: 0.7864031597971917\n",
      "Epoch: 442   Training Loss: 0.8210203779595239\n",
      "Epoch: 443   Training Loss: 0.8232804387807846\n",
      "Epoch: 444   Training Loss: 0.8373687539781843\n",
      "Epoch: 445   Training Loss: 0.8906315271343503\n",
      "Epoch: 446   Training Loss: 0.848157017145838\n",
      "Epoch: 447   Training Loss: 0.8919390588998795\n",
      "Epoch: 448   Training Loss: 0.884666354741369\n",
      "Epoch: 449   Training Loss: 0.818985435792378\n",
      "Epoch: 450   Training Loss: 0.7839052017245974\n",
      "Epoch: 451   Training Loss: 0.7852215988295419\n",
      "Epoch: 452   Training Loss: 0.7854755729436874\n",
      "Epoch: 453   Training Loss: 0.8078757813998632\n",
      "Epoch: 454   Training Loss: 0.8206800707748958\n",
      "Epoch: 455   Training Loss: 0.8512897670269013\n",
      "Epoch: 456   Training Loss: 0.867300859945161\n",
      "Epoch: 457   Training Loss: 0.8253818950482777\n",
      "Epoch: 458   Training Loss: 0.8659718947751182\n",
      "Epoch: 459   Training Loss: 0.8585171414273126\n",
      "Epoch: 460   Training Loss: 0.8012715096984591\n",
      "Epoch: 461   Training Loss: 0.8035101090158735\n",
      "Epoch: 462   Training Loss: 0.8649361457143511\n",
      "Epoch: 463   Training Loss: 0.9239851704665593\n",
      "Epoch: 464   Training Loss: 0.8637067671333041\n",
      "Epoch: 465   Training Loss: 0.8035002457244056\n",
      "Epoch: 466   Training Loss: 0.7796525163309914\n",
      "Epoch: 467   Training Loss: 0.8006137354033334\n",
      "Epoch: 468   Training Loss: 0.8628501440797534\n",
      "Epoch: 469   Training Loss: 0.8638192662170955\n",
      "Epoch: 470   Training Loss: 0.8782306909561157\n",
      "Epoch: 471   Training Loss: 0.8234489675079073\n",
      "Epoch: 472   Training Loss: 0.7629463129809925\n",
      "Epoch: 473   Training Loss: 0.7632449511970792\n",
      "Epoch: 474   Training Loss: 0.770801682131631\n",
      "Epoch: 475   Training Loss: 0.7652603409120015\n",
      "Epoch: 476   Training Loss: 0.8356316579239709\n",
      "Epoch: 477   Training Loss: 0.919481890967914\n",
      "Epoch: 478   Training Loss: 0.9181566298007965\n",
      "Epoch: 479   Training Loss: 0.8575112500361034\n",
      "Epoch: 480   Training Loss: 0.8498939450298036\n",
      "Epoch: 481   Training Loss: 0.8478533153023039\n",
      "Epoch: 482   Training Loss: 0.8061516642570495\n",
      "Epoch: 483   Training Loss: 0.8066033674137932\n",
      "Epoch: 484   Training Loss: 0.8192880979606083\n",
      "Epoch: 485   Training Loss: 0.8056065444435392\n",
      "Epoch: 486   Training Loss: 0.8049709239176341\n",
      "Epoch: 487   Training Loss: 0.7999515128987176\n",
      "Epoch: 488   Training Loss: 0.8453074825661523\n",
      "Epoch: 489   Training Loss: 0.870741308586938\n",
      "Epoch: 490   Training Loss: 0.9089504467589514\n",
      "Epoch: 491   Training Loss: 0.8644766505275454\n",
      "Epoch: 492   Training Loss: 0.8375236856085914\n",
      "Epoch: 493   Training Loss: 0.7854108035564422\n",
      "Epoch: 494   Training Loss: 0.7783266129238265\n",
      "Epoch: 495   Training Loss: 0.7832127311400005\n",
      "Epoch: 496   Training Loss: 0.782393279671669\n",
      "Epoch: 497   Training Loss: 0.8132308121238436\n",
      "Epoch: 498   Training Loss: 0.8190067708492279\n",
      "Epoch: 499   Training Loss: 0.9168168672493526\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LR = 1e-3\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR, weight_decay=5e-3)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 500\n",
    "losses = []\n",
    "\n",
    "for step in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inp, labels = data\n",
    "        inp, labels = inp.float(), labels.long()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inp)\n",
    "        cost = loss_func(outputs, labels)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += cost.item()\n",
    "    print(f'Epoch: {step}   Training Loss: {running_loss/len(train_loader)}')\n",
    "print('Training Complete')  \n",
    "\n",
    "\n",
    "\n",
    "# class CNNTextClassifier(nn.Module):\n",
    "#     def __init__ (self, oshape, ishape):\n",
    "#         super(CNNTextClassifier, self).__init__() \n",
    "#         self.conv1 = nn.Conv1d(ishape, 150)\n",
    "#         self.maxpool1 = nn.MaxPool1d(150)\n",
    "#         self.conv2 = nn.Conv1d(150, 100)\n",
    "#         self.maxpool2 = nn.MaxPool1d(100)\n",
    "#         self.conv3 = nn.Conv1d(100, 50)\n",
    "#         self.maxpool3 = nn.MaxPool1d(50)\n",
    "#         self.fc1 = nn.Linear(50, 25)\n",
    "#         self.fc2 = nn.Linear(25, oshape)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.maxpool1(F.relu(self.conv1(x)))\n",
    "#         x = self.maxpool2(F.relu(self.conv2(x)))\n",
    "#         x = self.maxpool3(F.relu(self.conv3(x)))\n",
    "#         x = x.flatten()\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.83288650580876%\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "acc = 0\n",
    "\n",
    "for i, data in enumerate(test_loader):\n",
    "    inp, labels = data\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = net(inp.float())\n",
    "    output = output.detach().numpy()\n",
    "    output = list(output)\n",
    "    output = [list(i).index(max(i)) for i in output]\n",
    "    \n",
    "    for idx, item in enumerate(torch.tensor(output)):\n",
    "        total += 1\n",
    "        if item == labels[idx]:\n",
    "            acc += 1\n",
    "print(f'{acc/total*100}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def format_text(token):\n",
    "#     clean_token = ''.join(chr for chr in token if chr.isalnum() and chr.isalpha())\n",
    "#     return clean_token\n",
    "\n",
    "\n",
    "# def tokenizer(data, text_cols, window=2):\n",
    "#     all_text = set()\n",
    "#     transformed_data = data\n",
    "#     for x in text_cols:\n",
    "#         for idx, entry in enumerate(data[x]):\n",
    "#             clean_entry = list(map(format_text, (word for word in entry)))\n",
    "#             append_all_text = set()\n",
    "#             for y, char in enumerate(clean_entry):\n",
    "#                 if char == '':\n",
    "#                     clean_entry[y] = ' '\n",
    "#             all_words = ''.join(i.lower() for i in clean_entry)\n",
    "#             transformed_data[x][idx] = all_words\n",
    "            \n",
    "#             for m in set(all_words.split(' ')):\n",
    "#                 all_text.add(m)\n",
    "\n",
    "#     return transformed_data, dict(zip(list(all_text), [z for z in range(len(all_text))]))\n",
    "            \n",
    "# truth_data = BiasDataset(data_dict['politifact'], DIRECTORY).data\n",
    "# truth_data_clean = BiasDataset(data_dict['politifact_clean'], DIRECTORY).data\n",
    "# truth_data_binary = BiasDataset(data_dict['politifact_clean_binarized'], DIRECTORY).data\n",
    "\n",
    "\n",
    "# truth_preprocessed, base_ref = tokenizer(truth_data_clean, ['statement'])\n",
    "# truth_preprocessed.head()\n",
    "# ##Custom Vector Embeddings\n",
    "# def word_vector(data, ref, text_cols):\n",
    "#     bag_dataset = data\n",
    "#     errors = []\n",
    "#     for row in text_cols:\n",
    "#         for idx, entry in enumerate(data[row]):\n",
    "#             list_entry = entry.split(' ')\n",
    "#             try:\n",
    "#                 vector = torch.FloatTensor([ref[word.lower()] for word in list_entry])\n",
    "#             except:\n",
    "#                 errors.append([list_entry, idx])\n",
    "#             bag_dataset[row][idx] = vector\n",
    "    \n",
    "#     return bag_dataset, errors\n",
    "\n",
    "\n",
    "\n",
    "# truth_data, errors = word_vector(truth_preprocessed.drop(['source', 'link'], axis=1), base_ref, ['statement'])\n",
    "# truth_data.head()\n",
    "\n",
    "# labels_check = list(truth_data['veracity'].unique())\n",
    "# ref_check = dict(zip(labels_check, [i for i in range(len(labels_check))]))\n",
    "# print(ref_check)\n",
    "\n",
    "# def vectorize(data_inp, columns):\n",
    "#     data = data_inp\n",
    "#     for column in columns:\n",
    "#         labels = list(truth_data[column].unique())\n",
    "#         ref = dict(zip(data[column].unique(), [i for i in range(len(labels))]))\n",
    "#         print(ref)\n",
    "#         for idx, val in enumerate(data[column]):\n",
    "#             vectorized = ref[data[column][idx]]\n",
    "#             data[column][idx] = torch.tensor(vectorized, dtype=float)\n",
    "#     return data\n",
    "\n",
    "# truth_processed = vectorize(truth_data, ['veracity'])\n",
    "\n",
    "# truth_processed.head()\n",
    "\n",
    "# truth_vector = truth_processed\n",
    "\n",
    "# max_len = max([len(i) for i in truth_vector['statement']])\n",
    "# for idx, i in enumerate(truth_vector['statement']):\n",
    "#     if len(i) != max_len:\n",
    "#         flag = False\n",
    "#         truth_vector['statement'][idx] = torch.cat((truth_vector['statement'][idx], torch.FloatTensor([0]*(max_len-len(i)))), 0)\n",
    "#         # print(len(truth_vector['statement'][idx]))\n",
    "#     else:\n",
    "#         flag = True\n",
    "\n",
    "# print(flag)\n",
    "# truth_vector.head() \n",
    "\n",
    "# processed_dataset = truth_vector\n",
    "\n",
    "# train_len = int(len(processed_dataset)*0.8)\n",
    "# test_len = len(processed_dataset) - train_len\n",
    "\n",
    "# train_dataset = processed_dataset.sample(n=train_len)\n",
    "# test_dataset = processed_dataset[~processed_dataset.isin(train_dataset)].dropna()\n",
    "# test_dataset.reset_index(inplace=True)\n",
    "# train_dataset.reset_index(inplace=True)\n",
    "\n",
    "# train_dataset = BiasDataset(train_dataset, DIRECTORY)\n",
    "# test_dataset = BiasDataset(test_dataset, DIRECTORY)\n",
    "\n",
    "# print(len(train_dataset))\n",
    "# print(len(test_dataset))\n",
    "# print(len(processed_dataset))\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
